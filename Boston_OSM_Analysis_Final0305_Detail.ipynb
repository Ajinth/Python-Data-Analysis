{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION \n",
    "\n",
    "I have been living in the Boston area for the last few years since grad school. The dataset analyzed for the purposes of this project pertains to the Boston area. The Boston area dataset was exported from [openstreetmaps](http://www.openstreetmap.org/#map=17/40.71652/-73.94470&layers=H). The analysis included the following steps \n",
    "\n",
    "* **Question Phase:** This phase involves asking general questions about the dataset. The questions involve the problem we are trying to solve for. \n",
    "* **Data Auditing:** This phase involves auditing the data to identify anomalies and patterns. E.g. In the streetmap data we could run into street names which have some kind special characters in them, or we could run into zipcodes in the Boston area that have some kind of alphabetical characters in them. \n",
    "* **Data Cleansing:** This phase involves classifying the anomalies that are identified in the previous step and devising approaches to clean up the data. The cleansing could be either manual or done programmatically. The project assumes both a programmatic and a manual approach to cleansing data. The focus is mostly been around cleansing the data programmatically. However in certain cases there is also a need for a manual review \n",
    "\n",
    "Data Auditing and Data Cleansing follow a repetive approach till a fair amount of data anomalies have been identified and also cleansed approrpriately. \n",
    "\n",
    "* **Conclusion:** This phase involves drawing conclusion about the dataset, based on the auditing and cleansing steps \n",
    "* **Communication:** The phase involves communicating the results of the analysis to the audiences. In a real life scenario this would be the business users who make business decisions based on the dataset analysis. \n",
    "\n",
    "In addition, this project also involves importing the dataset into [mongoDB](https://www.mongodb.com/), followed by executing some of the mongoDB's aggregation commands to further analyze the dataset that has been imported. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Phase\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Auditing\n",
    "## Identifying the TAGS along with the count of occurences of each of the TAGS\n",
    "\n",
    "This step involves doing an initial analysis of the dataset and doing an assessment of the XML nodes. The step also involves counting the number of instances of the specific node. While this step does provide a good start to the data auditing process, it does not answer a whole of questions that needs to be answered. This step definitely helps us confirm the validity of the XML format as the XML parser (ET.iterparse) is able to parse through the entire XML file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'node': 444899, 'nd': 551066, 'bounds': 1, 'member': 5284, 'tag': 221456, 'relation': 645, 'way': 75362, 'osm': 1})\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "filename = 'boston.osm'\n",
    "\n",
    "def test(): \n",
    "    tag_list = []\n",
    "    tag_dict = defaultdict(int)\n",
    "    for _, element in ET.iterparse(filename): \n",
    "        tag_list.append(element.tag)\n",
    "    for item in tag_list: \n",
    "        tag_dict[item] += 1\n",
    "    pprint.pprint(tag_dict)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Auditing (contd)\n",
    "## Identifying the Count of Unique Users (based on UIDs) \n",
    "OpenSteet map being an openly available map which can be updated by users all over the world, it made sense to get an idea of the number of unique users who have contributed to the map. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Users: 848\n",
      "Printing List of ten Unique User Ids: ['701372', '3057995', '378464', '14850', '967832', '152074', '4581744', '2176051', '113450', '45027']\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "filename = 'boston.osm'\n",
    "\n",
    "def test(): \n",
    "    users = set()\n",
    "    for _,element in ET.iterparse(filename):\n",
    "        if 'uid' in element.attrib:\n",
    "            users.add(element.get('uid'))\n",
    "    print \"Count of Unique Users:\", (len(users))\n",
    "# Commenting out the Line that prints out set of Unique Users \n",
    "#     print users\n",
    "    users_list = list(users)\n",
    "    print \"Printing List of ten Unique User Ids:\", users_list[:10]\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Auditing (contd)\n",
    "## Identifying the Count of Unique Users (based on Users) \n",
    "This test is very similar to the test done above, the only difference is that instead of using the UID element we are using the USER element. The test is done to be sure that the results are the same. In both the tests we notice that the counts are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "848\n",
      "Printing List of ten Unique Users: ['maxmetcalfe', 'Roger Neumann', 'dloutzen', 'TuftsReady', 'Matej Cepl', 'Steven Deeds', 'Brett Camper', 'noobi', 'Thia564', 'signed0']\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "filename = 'boston.osm'\n",
    "\n",
    "def test(): \n",
    "    users = set()\n",
    "    for _,element in ET.iterparse(filename):\n",
    "        if 'user' in element.attrib:\n",
    "            users.add(element.get('user'))\n",
    "    print len(users)\n",
    "    users_list = list(users)\n",
    "    print \"Printing List of ten Unique Users:\", users_list[:10]\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Auditing (Contd)\n",
    "## User Contribution Count:\n",
    "\n",
    "The purpose of this audit is to identify the number of times a specific user has contributed to the map. This test was done to just get an assessment of the top contribution numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('crschmidt', 269245),\n",
      " ('jremillard-massgis', 64989),\n",
      " ('wambag', 29490),\n",
      " ('OceanVortex', 27828),\n",
      " ('ryebread', 21770),\n",
      " ('morganwahl', 20412),\n",
      " ('mapper999', 8315),\n",
      " ('cspanring', 6817),\n",
      " ('JasonWoof', 5445),\n",
      " ('synack', 5054),\n",
      " ('ingalls_imports', 4166),\n",
      " ('Alexey Lukin', 3516),\n",
      " ('fiveisalive', 3145),\n",
      " ('MassGIS Import', 3115),\n",
      " ('Utible', 2872),\n",
      " ('probiscus', 1779),\n",
      " ('Prithason', 1439),\n",
      " ('phyzome', 1409),\n",
      " ('Extant', 1193),\n",
      " ('Alan Bragg', 1171),\n",
      " ('massDOT', 1167),\n",
      " ('Steven Deeds', 1124),\n",
      " ('pkoby', 1090),\n",
      " ('Ahlzen', 1080),\n",
      " ('thetornado76', 955),\n",
      " ('JessAk71', 953),\n",
      " ('3yoda', 890),\n",
      " ('jokeefe', 881),\n",
      " ('ceyockey', 790),\n",
      " ('woodpeck_repair', 771),\n",
      " ('Aredhel', 731),\n",
      " ('Pouletic', 702),\n",
      " ('jwass', 683),\n",
      " ('mterry', 669),\n",
      " ('dloutzen', 633),\n",
      " ('Peter Dobratz', 585),\n",
      " ('pokey', 574),\n",
      " ('KindredCoda', 553),\n",
      " ('dannya222', 546),\n",
      " ('aroach', 509),\n",
      " ('onurozgun', 504),\n",
      " ('headwatersolver', 464),\n",
      " ('kalanz', 410),\n",
      " ('nimapper', 394),\n",
      " ('Echo Echo', 393),\n",
      " ('nkhall', 382),\n",
      " ('spaceeinstein', 375),\n",
      " ('EricSJ', 353),\n",
      " ('calfarome', 353),\n",
      " ('gdt', 345),\n",
      " ('SophoM', 332),\n",
      " ('eugenebata', 317),\n",
      " ('Jim Kogler', 300),\n",
      " ('mregan', 278),\n",
      " ('DavidZ', 277),\n",
      " ('Parcanman', 277),\n",
      " ('techlady', 272),\n",
      " ('42429', 270),\n",
      " ('gsteinmon', 270),\n",
      " ('MDIV', 270),\n",
      " ('jremillard', 263),\n",
      " ('NE2', 254),\n",
      " ('ridixcr', 250),\n",
      " ('Ian McIntosh', 244),\n",
      " ('RMap1', 234),\n",
      " ('srevilak', 225),\n",
      " ('715371', 225),\n",
      " ('Jothirnadh', 221),\n",
      " ('Shannon Kelly', 221),\n",
      " ('steverumizen', 211),\n",
      " ('user_599436', 208),\n",
      " ('minewman', 205),\n",
      " ('Teole', 197),\n",
      " ('erjiang', 181),\n",
      " ('yurasi', 169),\n",
      " ('dannykath', 166),\n",
      " ('amillar', 162),\n",
      " ('wfox', 158),\n",
      " ('Iowa Kid', 156),\n",
      " ('flierfy', 154),\n",
      " ('Luis36995', 150),\n",
      " ('Larry Stone', 143),\n",
      " ('iandees', 140),\n",
      " ('oldtopos', 131),\n",
      " ('tirerim', 130),\n",
      " ('quantumwell', 126),\n",
      " ('Tim BL', 122),\n",
      " ('Niels Elgaard Larsen', 122),\n",
      " ('ljjwfr', 122),\n",
      " ('JeffMG', 120),\n",
      " ('StellanL', 116),\n",
      " ('dalek2point3', 113),\n",
      " ('wheelmap_visitor', 112),\n",
      " ('mh00', 109),\n",
      " ('maxerickson', 108),\n",
      " ('LinusA', 107),\n",
      " ('ShankarV', 106),\n",
      " ('OSMF Redaction Account', 103),\n",
      " ('JulienBalas', 100),\n",
      " ('claysmalley', 100),\n",
      " ('GeoStudent', 99),\n",
      " ('hopet', 98),\n",
      " ('Geogast', 97),\n",
      " ('Roger Zurawicki', 96),\n",
      " ('Craig Newell', 93),\n",
      " ('jacobolus', 91),\n",
      " ('piligab', 91),\n",
      " ('isabellekh', 90),\n",
      " ('user_1425650', 89),\n",
      " ('AlaskaDave', 87),\n",
      " ('Ron Newman', 85),\n",
      " ('PeterEastern', 83),\n",
      " ('agloe', 82),\n",
      " ('MikeN', 79),\n",
      " ('EvanMula', 79),\n",
      " ('giovanni berlanda', 77),\n",
      " ('rad1ance', 74),\n",
      " ('themiurgo', 73),\n",
      " ('jborthwick', 72),\n",
      " ('ewedistrict', 71),\n",
      " ('stevea', 71),\n",
      " ('MyWayOrNoHighway', 70),\n",
      " ('vkungys', 69),\n",
      " ('MaxVT', 66),\n",
      " ('RichRico', 65),\n",
      " ('TBHA', 65),\n",
      " ('Patrick Greenwell', 64),\n",
      " ('SK53', 64),\n",
      " ('Tom Walsh', 63),\n",
      " ('ediyes', 62),\n",
      " ('stevens', 62),\n",
      " ('breen', 62),\n",
      " ('Rouge568', 62),\n",
      " (\"Ethan O'Connor\", 61),\n",
      " ('karitotp', 61),\n",
      " ('marnen', 59),\n",
      " ('williamp', 58),\n",
      " ('TuftsReady', 57),\n",
      " ('Bill Witts', 57),\n",
      " ('Luis Capelo', 56),\n",
      " ('sankeytm', 55),\n",
      " ('sarhs440', 54),\n",
      " ('fx99', 54),\n",
      " ('Rory Nealon', 54),\n",
      " ('andrewpmk', 53),\n",
      " ('awoodruff', 53),\n",
      " ('robgeb', 52),\n",
      " ('Chanwoo', 52),\n",
      " ('pezespe', 51),\n",
      " ('Alan97', 51),\n",
      " ('jamessan', 51),\n",
      " ('phut', 50),\n",
      " ('garyg', 49),\n",
      " ('jinalfoflia', 49),\n",
      " ('Bryce C Nesbitt', 48),\n",
      " ('kzmijew', 48),\n",
      " ('bigspiral', 48),\n",
      " ('cowsandmilk', 47),\n",
      " ('Claumires', 47),\n",
      " ('wward', 45),\n",
      " ('Bill Ricker', 44),\n",
      " ('pratikyadav', 43),\n",
      " ('GoWestTravel', 43),\n",
      " ('samely', 43),\n",
      " ('SunetraB', 42),\n",
      " ('_Mathieu_', 42),\n",
      " ('ngallahe', 42),\n",
      " ('RD1', 42),\n",
      " ('NixG-D', 41),\n",
      " ('AMEDL', 40),\n",
      " ('nyuriks', 40),\n",
      " ('Anthony Moffa', 37),\n",
      " ('CitymapperHQ', 37),\n",
      " ('andygol', 37),\n",
      " ('jumbanho', 37),\n",
      " ('beweta', 36),\n",
      " ('Dishaan Ahuja', 36),\n",
      " ('bdiscoe', 36),\n",
      " ('jwsh', 35),\n",
      " ('EricTufts', 34),\n",
      " ('hidunno', 34),\n",
      " ('suuuuurge', 34),\n",
      " ('srividya_c', 32),\n",
      " ('Daniel Jalkut', 32),\n",
      " ('pbhade91', 32),\n",
      " ('Kent Johnson', 32),\n",
      " ('Tom Morris', 31),\n",
      " ('sirmmo', 31),\n",
      " ('leebier', 31),\n",
      " ('RussNelson', 30),\n",
      " ('PHerison', 30),\n",
      " ('bannus', 29),\n",
      " ('David Posey', 29),\n",
      " ('iSKUNK!', 28),\n",
      " ('keeeto', 28),\n",
      " ('RMap3', 28),\n",
      " ('radumas', 27),\n",
      " ('andreasviglakis', 27),\n",
      " ('Miriam R', 27),\n",
      " ('Paul Fisher', 26),\n",
      " ('kennedyneil', 26),\n",
      " ('Sat', 25),\n",
      " ('almiki', 25),\n",
      " ('charles92', 25),\n",
      " ('BeeeDeee', 25),\n",
      " ('WonderlustKing', 25),\n",
      " ('lmum', 25),\n",
      " ('KristenK', 25),\n",
      " ('dokam', 24),\n",
      " ('Doug0', 24),\n",
      " ('VMeyer', 24),\n",
      " ('PJorg', 24),\n",
      " ('eduardosl', 24),\n",
      " ('lckr', 22),\n",
      " ('farski', 22),\n",
      " ('One_Ironaut', 22),\n",
      " ('kellyb', 22),\n",
      " ('Maskulinum', 22),\n",
      " ('KEELEY6', 22),\n",
      " ('ChrisZontine', 21),\n",
      " ('cbIxDDJp6rQ', 21),\n",
      " ('dominastrum', 21),\n",
      " ('pgf', 21),\n",
      " ('Ryan Berdeen', 21),\n",
      " ('tjon', 21),\n",
      " ('Dr Kludge', 21),\n",
      " ('user_5359', 21),\n",
      " ('virtualxtc', 20),\n",
      " ('redsteakraw', 20),\n",
      " ('Phong Thai Cao', 20),\n",
      " ('oba510', 20),\n",
      " ('DrMORO', 20),\n",
      " ('dbzhao', 19),\n",
      " ('saikofish', 19),\n",
      " ('ansoncfit', 19),\n",
      " ('Ailish', 19),\n",
      " ('compdude', 19),\n",
      " ('sph2', 19),\n",
      " ('Trex2001', 19),\n",
      " ('David Quinn', 18),\n",
      " ('oini', 17),\n",
      " ('Andex', 17),\n",
      " ('PansenkinT', 17),\n",
      " ('Thia564', 16),\n",
      " ('scs', 16),\n",
      " ('John-Nicholas Furst', 16),\n",
      " ('osm2xp', 16),\n",
      " ('lakain', 16),\n",
      " ('jonaz', 15),\n",
      " ('Vova Kuznetsov', 15),\n",
      " ('bnewbold', 15),\n",
      " ('Saul Tannenbaum', 15),\n",
      " ('paulrosenzweig', 15),\n",
      " ('kinefuchi', 15),\n",
      " ('ajsalix', 15),\n",
      " ('nickdoesdevelopment', 15),\n",
      " ('FredRi', 14),\n",
      " ('Brian Stalder', 14),\n",
      " ('Benjamin Berklee', 14),\n",
      " ('drjat42', 14),\n",
      " ('Med', 14),\n",
      " ('Yoshinion', 14),\n",
      " ('RMap2', 14),\n",
      " ('Asumu Takikawa', 13),\n",
      " ('Sonzai', 13),\n",
      " ('anonymous58492', 13),\n",
      " ('NayanataraM', 13),\n",
      " ('Johnny Mapperseed', 13),\n",
      " ('Warren76', 13),\n",
      " ('grossing', 13),\n",
      " ('nck154', 13),\n",
      " ('wiso', 13),\n",
      " ('GeoservicesFDFA', 13),\n",
      " ('DanX', 13),\n",
      " ('krauszerr', 13),\n",
      " ('Roger Neumann', 12),\n",
      " ('pcs14', 12),\n",
      " ('xybot', 12),\n",
      " ('TheDimka', 12),\n",
      " ('Dan Wood', 12),\n",
      " ('tyrian', 12),\n",
      " ('nibr', 12),\n",
      " ('Chetan_Gowda', 12),\n",
      " ('bxhrz', 12),\n",
      " ('KaleenaSawyer', 12),\n",
      " ('maximerischard', 12),\n",
      " ('Brian Ristuccia', 12),\n",
      " ('0123456789', 11),\n",
      " ('Benito9', 11),\n",
      " ('cdkii', 11),\n",
      " ('DougPeterson', 11),\n",
      " ('Andrew Varnerin', 11),\n",
      " ('gradient_drift', 11),\n",
      " ('aonline1', 10),\n",
      " ('geodreieck4711', 10),\n",
      " ('jdm', 10),\n",
      " ('3ric', 10),\n",
      " ('korytaacheck', 10),\n",
      " ('tyos', 10),\n",
      " ('Benoit Thiell', 10),\n",
      " ('CloCkWeRX', 10),\n",
      " ('Chris Paci', 9),\n",
      " ('srajkovic', 9),\n",
      " ('dchiles', 9),\n",
      " ('winlong', 9),\n",
      " ('PlaneMad', 9),\n",
      " ('syzygyosm', 9),\n",
      " ('onlynone', 9),\n",
      " ('ThaBou', 9),\n",
      " ('macadoo212', 9),\n",
      " ('landfahrer', 9),\n",
      " ('aarthy', 9),\n",
      " ('Olyon', 8),\n",
      " ('abel801', 8),\n",
      " ('RetiredInNH', 8),\n",
      " ('kreycik', 8),\n",
      " ('Gone', 8),\n",
      " ('liryon', 8),\n",
      " ('mjfoster83', 8),\n",
      " ('BenCook', 8),\n",
      " ('paracetamolo', 8),\n",
      " ('mmaug', 8),\n",
      " ('Rollidave', 8),\n",
      " ('RobertBoston', 8),\n",
      " ('eemikula', 8),\n",
      " ('eric22', 8),\n",
      " ('BugBuster', 8),\n",
      " ('OSC', 8),\n",
      " ('Firozkhan3', 8),\n",
      " ('ScipioA', 7),\n",
      " ('lsweenstar', 7),\n",
      " ('KHGB', 7),\n",
      " ('gcamp', 7),\n",
      " ('youfu', 7),\n",
      " ('pluton_od', 7),\n",
      " ('MilaZ', 7),\n",
      " ('jonesydesign', 7),\n",
      " ('Owen Jennings', 7),\n",
      " ('StanB', 7),\n",
      " ('rockfender', 7),\n",
      " ('philipmolloy', 7),\n",
      " ('bburt33', 7),\n",
      " ('eriosw', 7),\n",
      " ('jcustin', 7),\n",
      " ('wdonovan', 7),\n",
      " ('Carl Seglem', 7),\n",
      " ('GerGel', 7),\n",
      " ('Hope Chen', 7),\n",
      " ('Jens Klein', 7),\n",
      " ('gavra', 7),\n",
      " ('Shanika Hettige', 7),\n",
      " ('kisaa', 7),\n",
      " ('TheC4pt', 7),\n",
      " ('Michael Hohensee', 7),\n",
      " ('boojum99', 7),\n",
      " ('ChrissW-R1', 7),\n",
      " ('JohnAKeith', 7),\n",
      " ('YunmoW', 6),\n",
      " ('mattbert', 6),\n",
      " ('afreeman', 6),\n",
      " ('zEEs', 6),\n",
      " ('ptaff', 6),\n",
      " ('hofoen', 6),\n",
      " ('Tomash Pilshchik', 6),\n",
      " ('PA94', 6),\n",
      " ('Gile Beye', 6),\n",
      " ('Michael Cutillo', 6),\n",
      " ('egore911', 6),\n",
      " ('Aldaron', 6),\n",
      " ('trmack2004', 6),\n",
      " ('Nothlit', 6),\n",
      " ('dlanznar', 6),\n",
      " ('hmvr', 6),\n",
      " ('Milton Bevington', 6),\n",
      " ('DavidSh', 6),\n",
      " ('irenedelatorre', 6),\n",
      " ('Abbe98', 6),\n",
      " ('amm', 6),\n",
      " ('sammadden', 6),\n",
      " ('werner2101', 6),\n",
      " ('Speight', 6),\n",
      " ('kgradow1', 6),\n",
      " ('shfishburn', 6),\n",
      " ('mapper117', 6),\n",
      " ('SGB', 6),\n",
      " ('wenhoe', 6),\n",
      " ('paolodepetrillo', 5),\n",
      " ('trsmith', 5),\n",
      " ('Eliyak', 5),\n",
      " ('RauvinJ', 5),\n",
      " ('jjkindy', 5),\n",
      " ('ereuss', 5),\n",
      " ('JasonM1', 5),\n",
      " ('Username2', 5),\n",
      " ('zenhack', 5),\n",
      " ('bbmiller', 5),\n",
      " ('Bob Leigh', 5),\n",
      " ('feranick', 5),\n",
      " ('fuzzyjoel', 5),\n",
      " ('Michael J Gilbert', 5),\n",
      " ('mzaa', 5),\n",
      " (u'\\u0410\\u043b\\u0435\\u043a\\u0441\\u0435\\u0439 \\u041a\\u043b\\u044e\\u0447\\u043d\\u0438\\u043a',\n",
      "  5),\n",
      " ('bawdy_bookworm', 5),\n",
      " ('mvfer', 5),\n",
      " ('hkelly', 5),\n",
      " ('Samuel Ekakurniawan', 5),\n",
      " ('Dero Bike Racks', 5),\n",
      " ('Sara Beth', 5),\n",
      " ('verhovzeva', 5),\n",
      " ('lipoff', 5),\n",
      " ('m3232', 5),\n",
      " ('kingd90', 5),\n",
      " ('tdtsystem1965', 5),\n",
      " ('jlicht', 5),\n",
      " ('HungryCharlie', 5),\n",
      " ('FrankCam', 5),\n",
      " ('tosseto', 5),\n",
      " ('jamacho', 5),\n",
      " ('sfn', 5),\n",
      " ('draiz89', 5),\n",
      " ('kirsanov', 5),\n",
      " ('eklee0120', 5),\n",
      " ('CycleStreets', 5),\n",
      " ('Paul Berry', 5),\n",
      " ('maxmetcalfe', 4),\n",
      " ('signed0', 4),\n",
      " ('pilotrobert', 4),\n",
      " ('wolfgang8741', 4),\n",
      " ('Sudip Chandra Paudel', 4),\n",
      " ('Holiday Inn Express Hotel & Suites Boston Garden', 4),\n",
      " ('marinero', 4),\n",
      " ('FHOResearch', 4),\n",
      " ('CarlYeks', 4),\n",
      " ('rhavens', 4),\n",
      " ('ruthmaben', 4),\n",
      " ('Federico Mena Quintero', 4),\n",
      " ('Steven Vance', 4),\n",
      " ('vVvA', 4),\n",
      " ('xunilOS', 4),\n",
      " ('Phippen', 4),\n",
      " ('Tobias Stundl', 4),\n",
      " ('jbecker85', 4),\n",
      " ('Brian Bellah', 4),\n",
      " ('bemasc', 4),\n",
      " ('bassettsj', 4),\n",
      " ('mvexel', 4),\n",
      " ('autonomy', 4),\n",
      " ('maggot27', 4),\n",
      " ('Stephen Peters', 4),\n",
      " ('Peter A', 4),\n",
      " ('Jonah', 4),\n",
      " ('BostonEnginerd', 4),\n",
      " ('dennismc', 4),\n",
      " ('T_9er', 4),\n",
      " ('Gregory Arenius', 4),\n",
      " ('mburt', 4),\n",
      " ('Hegewe01', 4),\n",
      " ('dhgoldberg', 4),\n",
      " ('gremio', 4),\n",
      " ('choess', 4),\n",
      " ('Mashin', 3),\n",
      " ('jak119', 3),\n",
      " ('Latze', 3),\n",
      " ('Jon Shea', 3),\n",
      " ('alojmm', 3),\n",
      " ('Evan82', 3),\n",
      " ('Maynewoods', 3),\n",
      " ('mosu84', 3),\n",
      " ('Michael Harnois', 3),\n",
      " ('richlv', 3),\n",
      " ('Jessica Allan Schmidt', 3),\n",
      " ('Rondale', 3),\n",
      " ('MorrisMK', 3),\n",
      " ('didi_o', 3),\n",
      " ('craftsbury', 3),\n",
      " ('gknisely', 3),\n",
      " ('joshk', 3),\n",
      " ('Sigi Reich', 3),\n",
      " ('viiskis', 3),\n",
      " ('Pete Robie', 3),\n",
      " ('Christopher Beland', 3),\n",
      " ('Jeremy Quanno', 3),\n",
      " ('Aleks-Berlin', 3),\n",
      " ('Jimber', 3),\n",
      " ('imbw267', 3),\n",
      " ('sejohnson', 3),\n",
      " ('bucs3282', 3),\n",
      " ('slw2014', 3),\n",
      " ('jkw', 3),\n",
      " ('cmurtaugh', 3),\n",
      " ('Carlos Tirado', 3),\n",
      " ('hlieberman', 3),\n",
      " ('JuanBorre', 3),\n",
      " ('wsloand', 3),\n",
      " ('salix01', 3),\n",
      " ('Blobo123', 3),\n",
      " ('jthandle', 3),\n",
      " ('AndiG88', 3),\n",
      " ('basalt51', 3),\n",
      " ('thomergil', 3),\n",
      " ('meggle', 3),\n",
      " ('sujanrajjoshi', 3),\n",
      " ('Rub21', 3),\n",
      " ('Hawkeye', 3),\n",
      " ('Matej Cepl', 2),\n",
      " ('Brett Camper', 2),\n",
      " ('Feddy Pariona Rojas', 2),\n",
      " ('cgu66', 2),\n",
      " ('Manu1400', 2),\n",
      " ('moyogo', 2),\n",
      " ('smita1', 2),\n",
      " ('Ivanaf', 2),\n",
      " ('dfieldsarlington', 2),\n",
      " ('randomintsolo', 2),\n",
      " ('pollyanna', 2),\n",
      " ('uboot', 2),\n",
      " ('thisss', 2),\n",
      " ('Marcus PS', 2),\n",
      " ('grtm', 2),\n",
      " ('Ropino', 2),\n",
      " (u'Vincent D\\xe9m', 2),\n",
      " ('Evan Jones', 2),\n",
      " ('ybensadoun', 2),\n",
      " (u'Walter Schl\\xf6gl', 2),\n",
      " ('davidearl', 2),\n",
      " ('Paul Knight', 2),\n",
      " ('Curodo', 2),\n",
      " ('daysigomez13', 2),\n",
      " ('patch615', 2),\n",
      " ('Raj Singh', 2),\n",
      " ('cmlja', 2),\n",
      " ('Darqonomous', 2),\n",
      " ('Bobby-Fischer', 2),\n",
      " ('Raq929', 2),\n",
      " ('tixuwuoz', 2),\n",
      " ('gregsharp', 2),\n",
      " ('SveNss0N', 2),\n",
      " ('must1n', 2),\n",
      " ('dedwards8', 2),\n",
      " ('Harry Cutts', 2),\n",
      " ('lesko987', 2),\n",
      " ('adjuva', 2),\n",
      " ('dxanato', 2),\n",
      " ('mbiker', 2),\n",
      " ('sanschag', 2),\n",
      " ('AbaddonPR', 2),\n",
      " ('asmithmd1', 2),\n",
      " ('RDP3', 2),\n",
      " ('mackerski', 2),\n",
      " ('Syl', 2),\n",
      " ('HolgerJeromin', 2),\n",
      " ('mazugrin', 2),\n",
      " ('joelbikesalot', 2),\n",
      " ('Manfredo Corado', 2),\n",
      " ('RRizman', 2),\n",
      " ('novikoffav', 2),\n",
      " ('blablubb1234', 2),\n",
      " ('chunkywater', 2),\n",
      " ('tko', 2),\n",
      " ('osm-sputnik', 2),\n",
      " ('Test360', 2),\n",
      " ('Matthew Miller', 2),\n",
      " ('clairehhlin', 2),\n",
      " ('dantje', 2),\n",
      " ('theavclub', 2),\n",
      " ('Sean-of-the-GIS', 2),\n",
      " ('James Michael DuPont', 2),\n",
      " ('Constable', 2),\n",
      " ('Anders Brownworth', 2),\n",
      " ('hno2', 2),\n",
      " ('atannen', 2),\n",
      " ('cosmicduck', 2),\n",
      " ('Rohan Mehra', 2),\n",
      " ('john abbott', 2),\n",
      " ('nassive palmer', 2),\n",
      " ('tylerritchie', 2),\n",
      " ('myersj', 2),\n",
      " ('van Rees', 2),\n",
      " ('CRichmond', 2),\n",
      " ('dru1138', 2),\n",
      " ('rorybecker', 2),\n",
      " ('rodonn', 2),\n",
      " ('ioptio', 2),\n",
      " ('Claudius Henrichs', 2),\n",
      " ('GrollTech', 2),\n",
      " ('Katie Baker', 2),\n",
      " ('Markus59', 2),\n",
      " ('celosia', 2),\n",
      " ('youngbasedallah', 2),\n",
      " ('woodpeck', 2),\n",
      " ('motlib', 2),\n",
      " ('NE3', 2),\n",
      " ('H4rr1s0n', 2),\n",
      " ('TomHynes', 2),\n",
      " ('Sarvihepo', 2),\n",
      " ('FrViPofm', 2),\n",
      " ('noobi', 1),\n",
      " ('genuinejack', 1),\n",
      " ('jraviles', 1),\n",
      " ('ramyaragupathy', 1),\n",
      " ('pierlux', 1),\n",
      " ('Miselajus', 1),\n",
      " ('gameboo', 1),\n",
      " ('digdesign', 1),\n",
      " ('tmcw', 1),\n",
      " ('sivan00', 1),\n",
      " ('smithbone', 1),\n",
      " ('Htg610', 1),\n",
      " ('lm0nster', 1),\n",
      " ('Joshua Gerber', 1),\n",
      " ('db248', 1),\n",
      " ('Manuel Aristaran', 1),\n",
      " ('kumarhk', 1),\n",
      " ('Carnildo', 1),\n",
      " ('willber118', 1),\n",
      " ('Roadsguy', 1),\n",
      " ('jforbess', 1),\n",
      " ('mbourqui', 1),\n",
      " ('cambridgecleanersm', 1),\n",
      " ('1248', 1),\n",
      " (u'Beno\\xeet Prieur', 1),\n",
      " ('bmyren', 1),\n",
      " ('Ben Alvord', 1),\n",
      " ('Gabriel Ehrnst Grundin', 1),\n",
      " ('skorbut', 1),\n",
      " ('Chris King', 1),\n",
      " ('Nasrath Faisal', 1),\n",
      " ('matthieun', 1),\n",
      " ('hanoj', 1),\n",
      " ('Dave Breeding', 1),\n",
      " ('JamesFNomar', 1),\n",
      " ('TWRE', 1),\n",
      " ('mrw6060', 1),\n",
      " ('richardpetithory', 1),\n",
      " ('StreamingMeemee', 1),\n",
      " ('ettob', 1),\n",
      " ('arajewelers', 1),\n",
      " ('h4ck3rm1k3', 1),\n",
      " ('nfgusedautoparts', 1),\n",
      " ('simlox', 1),\n",
      " ('nicole_thalia', 1),\n",
      " ('buna', 1),\n",
      " ('ethelmermaid', 1),\n",
      " ('rmikke', 1),\n",
      " ('dbaron', 1),\n",
      " ('Glassman', 1),\n",
      " ('ChogyDan', 1),\n",
      " ('Mafketel', 1),\n",
      " ('marcodena', 1),\n",
      " ('aroundi', 1),\n",
      " ('VinceTraveller', 1),\n",
      " ('Jake Strine', 1),\n",
      " ('mueschel', 1),\n",
      " ('Bill Allen', 1),\n",
      " ('ganka', 1),\n",
      " ('amadels', 1),\n",
      " ('sudozero', 1),\n",
      " ('EchoDitto', 1),\n",
      " ('smacmillan', 1),\n",
      " ('domdomegg', 1),\n",
      " ('thazel', 1),\n",
      " ('njaard', 1),\n",
      " ('theadkgroup', 1),\n",
      " ('irumac00', 1),\n",
      " ('Stemby', 1),\n",
      " ('Dami_Tn', 1),\n",
      " ('winsto99', 1),\n",
      " ('ubermonkey79', 1),\n",
      " ('Copley Square Hotel', 1),\n",
      " ('abschiff', 1),\n",
      " ('yngmthrs', 1),\n",
      " ('teddythebeta', 1),\n",
      " ('Sanjak', 1),\n",
      " ('margonotmango', 1),\n",
      " ('Unusual User Name', 1),\n",
      " ('Alan Trick', 1),\n",
      " ('Retail Technology Group', 1),\n",
      " ('akweykan', 1),\n",
      " ('Higonnet', 1),\n",
      " ('rebeccaboofox', 1),\n",
      " ('quentin', 1),\n",
      " ('anarcat', 1),\n",
      " ('redsquareblack', 1),\n",
      " ('elbatrop', 1),\n",
      " ('Jonathan ZHAO', 1),\n",
      " ('BradBarnett', 1),\n",
      " ('Chris Rodger', 1),\n",
      " ('Gregory Boyce', 1),\n",
      " ('roderickb', 1),\n",
      " ('ejegg', 1),\n",
      " ('Greg Johnston', 1),\n",
      " ('wakeldan', 1),\n",
      " ('MetaMonk', 1),\n",
      " ('Ethan Stern', 1),\n",
      " ('YamaOfParadise', 1),\n",
      " ('millerwachman', 1),\n",
      " ('drbobx', 1),\n",
      " ('Jason Carreiro', 1),\n",
      " ('MJFC', 1),\n",
      " ('Alps Pierrat', 1),\n",
      " ('rosieks', 1),\n",
      " ('Wololo', 1),\n",
      " ('ubermonkey', 1),\n",
      " ('Jano John Akim Franke', 1),\n",
      " ('bleesand', 1),\n",
      " ('Sarkis Shirinyan', 1),\n",
      " ('mikexstudios', 1),\n",
      " ('Hayboy', 1),\n",
      " ('colindt', 1),\n",
      " ('The Big O Face', 1),\n",
      " ('TylerMills', 1),\n",
      " ('greta', 1),\n",
      " ('shravan91', 1),\n",
      " ('Bill McAvinney', 1),\n",
      " ('MahaNM', 1),\n",
      " ('Rightlegpegged', 1),\n",
      " ('colorcraft', 1),\n",
      " ('Travis Bashir', 1),\n",
      " ('Omnific', 1),\n",
      " ('Christoph Lotz', 1),\n",
      " ('theurbanmapper', 1),\n",
      " ('Cartophiliac', 1),\n",
      " ('schoolofgroove', 1),\n",
      " ('djsnaxz', 1),\n",
      " ('ipartola', 1),\n",
      " ('geozeisig', 1),\n",
      " ('mprojekt', 1),\n",
      " ('Louis Galvez III', 1),\n",
      " ('brogo', 1),\n",
      " ('SCSpaeth', 1),\n",
      " ('ighare', 1),\n",
      " ('Faith Pastor', 1),\n",
      " ('Mike Linksvayer', 1),\n",
      " ('cdavila', 1),\n",
      " ('TarlaMoede', 1),\n",
      " ('evanlgray', 1),\n",
      " ('Charles Bandes', 1),\n",
      " ('Evan Danaher', 1),\n",
      " ('Michael Williams', 1),\n",
      " ('jc67', 1),\n",
      " ('Eyas', 1),\n",
      " ('Dave330', 1),\n",
      " ('Jac Beattie', 1),\n",
      " ('nickizd', 1),\n",
      " ('cuttothechasse', 1),\n",
      " ('FloppusMaximus', 1),\n",
      " ('rickmastfan67', 1),\n",
      " ('Yunjie', 1),\n",
      " ('wmann', 1),\n",
      " ('brianegge', 1),\n",
      " ('JeffTakle', 1),\n",
      " ('SomeoneElse_Revert', 1),\n",
      " ('MichelleM123', 1),\n",
      " ('EricBell', 1),\n",
      " ('emacsen', 1),\n",
      " ('Janjko', 1),\n",
      " ('mapmeld', 1),\n",
      " ('Gordon2485', 1),\n",
      " ('GregCh', 1),\n",
      " ('dph', 1),\n",
      " ('LogicalViolinist', 1),\n",
      " ('ravn', 1),\n",
      " ('Valentino-46', 1),\n",
      " ('borazslo', 1),\n",
      " ('alester', 1),\n",
      " ('erik schlegel', 1),\n",
      " ('jleedev', 1),\n",
      " ('kxra', 1),\n",
      " ('ITMarketweb', 1),\n",
      " ('cmayne', 1),\n",
      " ('FedericoCozzi', 1),\n",
      " ('zacmccormick', 1),\n",
      " ('rjmunro', 1),\n",
      " ('tuttlesclean', 1),\n",
      " ('Torfason', 1),\n",
      " ('Cato_d_Ae', 1),\n",
      " ('SWF8', 1),\n",
      " ('nygren', 1),\n",
      " ('Andre Engels', 1),\n",
      " ('MrtnMcc', 1),\n",
      " ('elle_dubs', 1),\n",
      " ('VKW', 1),\n",
      " ('gutelius', 1),\n",
      " ('doak', 1),\n",
      " ('Mackenzi Coan', 1),\n",
      " ('CambridgeRes', 1),\n",
      " ('Dean Covert', 1),\n",
      " ('Uncle Mango', 1),\n",
      " ('Thibaut75011', 1),\n",
      " ('massimo_tassi', 1),\n",
      " ('jlev', 1),\n",
      " ('Josef73', 1),\n",
      " ('jfarid27', 1),\n",
      " ('Ian Connor', 1),\n",
      " ('npettiaux', 1),\n",
      " ('TomStopka', 1),\n",
      " ('cditze', 1),\n",
      " ('Davlak', 1),\n",
      " (u'Kom\\u044fpa', 1),\n",
      " ('iHead', 1),\n",
      " ('Campus Planner', 1),\n",
      " ('inertial_circles', 1),\n",
      " ('drsewell426', 1),\n",
      " ('wandsecacher', 1),\n",
      " ('Flitzpiepe', 1),\n",
      " ('HHilton', 1),\n",
      " ('danrassi', 1),\n",
      " ('wlgann', 1),\n",
      " ('jgt', 1),\n",
      " ('steinmi', 1),\n",
      " ('SP!KE', 1),\n",
      " ('ervano', 1),\n",
      " ('wvdp', 1),\n",
      " ('Shred Nations', 1),\n",
      " ('Zandlopertje', 1),\n",
      " ('brandlebob', 1),\n",
      " ('Evan Morikawa', 1),\n",
      " ('arjunkmohan', 1),\n",
      " ('emilypbernier', 1),\n",
      " ('reverend_paco', 1),\n",
      " ('tamaa', 1),\n",
      " ('Hyatt Regency Boston', 1),\n",
      " ('Kurly', 1),\n",
      " ('The Liberty Hotel', 1),\n",
      " (u'Dauerl\\xe4ufer', 1),\n",
      " ('tomk11', 1),\n",
      " ('mcgrathj', 1),\n",
      " ('bigfoolio', 1),\n",
      " ('GerdP', 1),\n",
      " ('cammls', 1),\n",
      " ('MichaelTerner', 1),\n",
      " ('jdalco', 1),\n",
      " ('Pmz', 1),\n",
      " ('ndavidow', 1),\n",
      " ('Gabridoodle', 1),\n",
      " ('bkrausz', 1),\n",
      " ('R0bst3r', 1),\n",
      " ('JP-Motus', 1),\n",
      " ('jcd', 1),\n",
      " ('ari277', 1),\n",
      " ('pcaurorep', 1),\n",
      " ('mkcabgfx', 1),\n",
      " ('Joe Carreiro', 1),\n",
      " ('Shidash', 1),\n",
      " ('unitelcompany', 1),\n",
      " ('ssbostonmap', 1),\n",
      " ('t-i', 1),\n",
      " ('makoshark', 1)]\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import codecs\n",
    "import json\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "filename = 'boston.osm'\n",
    "\n",
    "def test(): \n",
    "    user_list = []\n",
    "    user_dict = defaultdict(int)\n",
    "    for _,element in ET.iterparse(filename):\n",
    "        if 'user' in element.attrib:\n",
    "            user_list.append(element.get('user'))\n",
    "        else: \n",
    "            continue\n",
    "    \n",
    "    for item in user_list: \n",
    "        user_dict[item] +=1 \n",
    "    sorted_user_dict = sorted(user_dict.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    pprint.pprint(sorted_user_dict)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The top ten contributors based on the results of the output above:**\n",
    "[('crschmidt', 269245),\n",
    " ('jremillard-massgis', 64989),\n",
    " ('wambag', 29490),\n",
    " ('OceanVortex', 27828),\n",
    " ('ryebread', 21770),\n",
    " ('morganwahl', 20412),\n",
    " ('mapper999', 8315),\n",
    " ('cspanring', 6817),\n",
    " ('JasonWoof', 5445),\n",
    " ('synack', 5054)]\n",
    " \n",
    " In addition did some research on crschmidt and found this (http://crschmidt.net/mapping/). Looks like the user has been actively contributing to the online map community via series of hacks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Auditing (contd)\n",
    "## This step involves auditing the value of the addr tags for \n",
    "\n",
    "1. Lower Case Characters \n",
    "2. Lower Case Characters with Colon \n",
    "3. Problematic Characters \n",
    "\n",
    "The final output includes a dictionary with keys for each of the category and the count of values for each of the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'int'>, {'problemchars': 3332, 'lower': 83, 'other': 8195})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "filename = 'boston.osm'\n",
    "\n",
    "\n",
    "def process_address(addr_value):\n",
    "    if re.search(lower,addr_value):\n",
    "        keys['lower'] += 1\n",
    "    elif re.search(lower_colon, addr_value): \n",
    "        keys['lower_colon'] += 1\n",
    "    elif re.search(problemchars, addr_value): \n",
    "        keys['problemchars'] += 1 \n",
    "    else: \n",
    "        keys['other'] += 1\n",
    "                \n",
    "\n",
    "def read_file():\n",
    "    for _, element in ET.iterparse(filename): \n",
    "        if element.tag == 'tag': \n",
    "            key=element.get('k')\n",
    "            if 'addr:' in key:\n",
    "                addr_value = element.get('v')\n",
    "                process_address(addr_value)\n",
    "    return keys\n",
    "                    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    keys = defaultdict(int)\n",
    "    read_key = read_file()\n",
    "    print read_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Auditing (contd) \n",
    "## Auditing Street Names and Identifying Anomalies\n",
    "This step involves auditing the street names for the last element in the string name. Identifying the last element in street name is done via regular expression. \n",
    "\n",
    "The psuedocode involves comparing this \"last element value\" in the street name with list of expected elements in the \"expected\" list. If the \"last element value\" is not found in the expected list, the element is added to the dictionary as key along with the associated street name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1100': set(['First Street, Suite 1100']),\n",
      " '1302': set(['Cambridge Street #1302']),\n",
      " '3': set(['Kendall Square - 3']),\n",
      " '303': set(['First Street, Suite 303']),\n",
      " '501': set(['Bromfield Street #501']),\n",
      " '6': set(['South Station, near Track 6']),\n",
      " '846028': set(['PO Box 846028']),\n",
      " 'Ave': set(['738 Commonwealth Ave',\n",
      "             'Boston Ave',\n",
      "             'College Ave',\n",
      "             'Commonwealth Ave',\n",
      "             'Concord Ave',\n",
      "             'Francesca Ave',\n",
      "             'Highland Ave',\n",
      "             'Josephine Ave',\n",
      "             'Lexington Ave',\n",
      "             'Massachusetts Ave',\n",
      "             'Morrison Ave',\n",
      "             'Mystic Ave',\n",
      "             'Somerville Ave',\n",
      "             'Western Ave',\n",
      "             'Willow Ave']),\n",
      " 'Ave.': set(['Brighton Ave.',\n",
      "              'Massachusetts Ave.',\n",
      "              'Somerville Ave.',\n",
      "              'Spaulding Ave.']),\n",
      " 'Broadway': set(['Broadway']),\n",
      " 'Cambrdige': set(['Cambrdige']),\n",
      " 'Center': set(['Cambridge Center', 'Financial Center']),\n",
      " 'Circle': set(['Norcross Circle']),\n",
      " 'Ct': set(['Kelley Ct']),\n",
      " 'Driveway': set(['Museum of Science Driveway']),\n",
      " 'Elm': set(['Elm']),\n",
      " 'Floor': set(['Boylston Street, 5th Floor']),\n",
      " 'Hall': set(['Faneuil Hall']),\n",
      " 'Hampshire': set(['Hampshire']),\n",
      " 'Highway': set([\"Monsignor O'Brien Highway\", 'Santilli Highway']),\n",
      " 'Holland': set(['Holland']),\n",
      " 'Hwy': set([\"Monsignor O'Brien Hwy\"]),\n",
      " 'LEVEL': set(['LOMASNEY WAY, ROOF LEVEL']),\n",
      " 'Lafayette': set(['Avenue De Lafayette']),\n",
      " 'Mall': set(['Cummington Mall']),\n",
      " 'Market': set(['Faneuil Hall Market']),\n",
      " 'Newbury': set(['Newbury']),\n",
      " 'Park': set(['Acorn Park',\n",
      "              'Austin Park',\n",
      "              'Canal Park',\n",
      "              'Exeter Park',\n",
      "              'Giles Park',\n",
      "              'Granton Park']),\n",
      " 'Pkwy': set(['Birmingham Pkwy']),\n",
      " 'Pl': set(['Longfellow Pl']),\n",
      " 'Plaza': set(['Two Center Plaza']),\n",
      " 'Rd': set(['Abby Rd', 'Aberdeen Rd', 'Bristol Rd', 'Soldiers Field Rd']),\n",
      " 'Row': set(['Assembly Row', 'Professors Row']),\n",
      " 'ST': set(['Newton ST']),\n",
      " 'Sq.': set(['1 Kendall Sq.']),\n",
      " 'St': set(['1629 Cambridge St',\n",
      "            'Antwerp St',\n",
      "            'Arsenal St',\n",
      "            'Athol St',\n",
      "            'Bagnal St',\n",
      "            'Brentwood St',\n",
      "            'Cambridge St',\n",
      "            'Charles St',\n",
      "            'Cummington St',\n",
      "            'Dane St',\n",
      "            'Duval St',\n",
      "            'Elm St',\n",
      "            'Everett St',\n",
      "            'Hampshire St',\n",
      "            'Holton St',\n",
      "            'Kirkland St',\n",
      "            'Leighton St',\n",
      "            'Litchfield St',\n",
      "            'Lothrop St',\n",
      "            'Mackin St',\n",
      "            'Main St',\n",
      "            'Merrill St',\n",
      "            'Mt Auburn St',\n",
      "            'N Beacon St',\n",
      "            'Norfolk St',\n",
      "            'Portsmouth St',\n",
      "            'Richardson St',\n",
      "            'South Waverly St',\n",
      "            'Ware St',\n",
      "            'Waverly St',\n",
      "            'Winter St']),\n",
      " 'St,': set(['Walnut St,']),\n",
      " 'St.': set(['Albion St.',\n",
      "             'Banks St.',\n",
      "             'Boylston St.',\n",
      "             'Elm St.',\n",
      "             'Main St.',\n",
      "             'Marshall St.',\n",
      "             'Pearl St.',\n",
      "             'Prospect St.',\n",
      "             \"Saint Mary's St.\",\n",
      "             'Stuart St.']),\n",
      " 'Terrace': set(['Alberta Terrace', 'Norfolk Terrace']),\n",
      " 'Way': set(['Artisan Way',\n",
      "             'Cedar Lane Way',\n",
      "             'David G Mugar Way',\n",
      "             'Harry Agganis Way',\n",
      "             'Memorial Way',\n",
      "             'Yawkey Way']),\n",
      " 'Windsor': set(['Windsor']),\n",
      " 'Winsor': set(['Winsor']),\n",
      " 'floor': set(['First Street, 18th floor', 'Sidney Street, 2nd floor']),\n",
      " 'place': set(['argus place']),\n",
      " 'st': set(['Main st'])}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Heights\", \"North\", \"East\", \"West\", \"South\"]\n",
    "\n",
    "\n",
    "filename = 'boston.osm'\n",
    "                \n",
    "def process_addressanomalies(addressvalue): \n",
    "    match = street_type_re.search(addressvalue)\n",
    "    if match: \n",
    "        street_type = match.group()\n",
    "        if street_type not in expected: \n",
    "            street_types[street_type].add(addressvalue)\n",
    "        \n",
    "def read_file():\n",
    "    for _, element in ET.iterparse(filename): \n",
    "        if element.tag == 'tag': \n",
    "            key=element.get('k')\n",
    "            if 'addr:street' in key:\n",
    "                addr_value = element.get('v')\n",
    "                process_addressanomalies(addr_value)\n",
    "    return street_types                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    street_types = defaultdict(set)\n",
    "    read_key = read_file()\n",
    "pprint.pprint(dict(read_key))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On quick review of the output from the previous step we see that \n",
    "1. Street has different variations. E.g. Some forms include \"st\", \"St\", \"St.\" , \"St,\" and also \"ST\" \n",
    "2. Similarly we also see that Avenue is sometimes referred to as \"Ave\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Auditing (contd) \n",
    "## Auditing the Phone Numbers and Identifying Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424\n",
      "set(['+1 617 876-3988', '617-261-0158', '+1 617 576 1253', '+1 (781) 267-4539', '(617) 277-3737', '617-254-7163', '+1-617-294-4233', '6174894408', '(617) 206-2994', '617 491 2999', '+1 617 375 8550', '+1-617-764-3152', '+16177872430', '617-635-8937', '+1 617 661 0077', '+1 (617) 623-9068', '+1 617 876 6990', '617 236 0571', '(617) 863-3650', '+1 617 440 4192', '+16176610433', '+1 (857) 417-2396', '+1 617 227-2750', '(617) 307-7608', '+1 617 4971513', '+1 617 876-6555', '+1 617 349 3937', '+1 617 868-6330', '+1 617 262 2424', '(617) 714-3974', '(866) 995-2479', '+1 617 496-5955', '+1-617-627-3030', '+1 617 628-3618', '617-926-7740', '+16175420210', '+1-857-242-3605', '+1-617-542-5942', '+1 617 254-0112', '+1 617 441-2500', '+16174920711', '+1 857 2596552', '+1 617 987-4236', '617-787-5507', '+1 617 623 1159', '(617) 494-8700', '+1-617-236-1100', '+26777722147', '18006350489', '617-225-2777', '+1 617 783-5804', '+1 617 497 3926', '+16173496567', '617-635-6470', '+1 617 764 4960', '617.253.1000', '617-284-7800', '617-623-3447', '+1 617 864-1300', '617-635-9914', '+16178646100', '+16176617411', '+16174971221', '+1 617 547 7788', '+1 617 868 4140', '617 868-3000', '+1-617-783-5100', '+16178689400', '+16177588495', '+16174922541', '+1 617 547 6000', '+16175766779', '+1 617 354 9523', '617-576-3000', '+1 617 625-1793', '+1-617-623-4378', '+1 866 228 6439', '+1 617 547 1234', '(617) 868-8658', '781-393-5600', '617-422-5898', '+1 617 354 8582', '+1 617 628-0328', '+1 617 5221966', '+1 617 623 0867', '+1 617 337 4088', '+1 857 263 8491', '+1 617 868 6740', '617-266-5999', '+1 617 945 0878', '+1 (781) 464-2000', '+16175766260', '781-316-3782', '+1 617 441 0537', '+1-857-999-2003', '617-491-6616', '617-547-2727', '+16172628900', '+1 (617) 482-3473', '6173671866', '+1 617 236 0990', '+1 6175361775', '617-566-7730', '(617) 300-3400', '(617) 492-7289', '+1 617 349-1650', '+16174076271', '617-354-7766', '+1 617 776-2011', '617-349-6588', '+1-617-539-8570', '617-625-0700', '1-617-227-3962', '+1-617-491-1200', '617.498.0020', '+1 617 492 9030', '+1 617 666 1790', '+16175470836', '+1-617-491-9638', '617-742-0520', '+16178763601', '617-635-8358', '617-349-6632', '1-855-305-2347', '+16176255700', '+18006302521', '+1 617 494-1994', '+1-617-782-7625', '+1 617 3496314', '617-284-6878', '6173752524', '617-267-4530', '617-426-2121', '+1-617-666-4444', '+16173543600', '617-635-9911', '+1 (617) 868-3392', '617-266-7480', '+1-617-424-1010', '781-646-5598', '6177205740', '617.431.7250', '+16178688800', '617 523 0453', '617-679-5500', '617-993-5500', '6173389788', '617-349-6841', '(617) 349-4015', '+16178764853', '+1 617 4412116', '617.354.6363', '+16178679300', '617-247-3399', '+1 617 945 9656', '+1 617 764-5365', '(617) 871-9911', '781-641-5992', '617-482-7467', '+1 617 523-0990', '1-617-209-2257', '+1-617-868-4200', '+1-617-661-8049', '(617) 401-3390', '+16175366300', '617-227-0150', '+1-617-202-5817', '617-879-4400', '+1 617 666 3830', '617-349-6555', '781-393-2177', '+1-617-936-2285', '+1 617 576 3229', '6176661079', '(617) 491-3400', '(617) 987-0086', '+1-617-764-4143', '+1 617 491-3405', '+1 617 354 3388', '617-679-7000', '617-868-1260', '+1 617 547 2455', '+16172997405', '617-635-8497', '+16174911160', '+1 617 4927540', '617-349-6600', '617-776-3596', '617-993-5800', '+1 617 575 8700', '+1 617 354 9898', '+16172472818', '617-500-9373', '+1 617 6664200', '+1 866 7740879', '+1 617 876 3076', '(617) 577-7427', '(617) 868-9560', '617-666-3900', '+1 781 777-1451', '+1 617 492 0870', 'Phone 617.714.0555', '617-354-5410', '+1 617 612 8253', '+16178761655', '6179243068', '+16174416999', '617-254-8383', '617-491-2220', '617-739-1794', '6174247000', '617-547-6100', '+16173545287', '(617) 247-4141', '617-389-2448', '+16173541441', '(617) 349-4023', '(617) 349-4021', 'AstraZeneca Neuroscience: T: (617) 679-1680', '617-731-5330', '+16175763634', '+1 617 492-5588', '781-393-2333', '617 776 4036', '+18572596888', '617-242-2080', '+1 617-547-1595', '+1 617 718-0602', '+1 617 236-0752', '6172414999', '617-536-3355', '+1 617 232-0446', '+1 617 576 5300', '+1 617 349 0700', '617-993-0750', '+1 617 623-5000', '+1 617-734-7708', '616-225-0409', '1-617-437-7700', '(617) 621-9902', '617-372-8156', '617-635-8865', '+16174927777', '617-451-2289', '617-500-6778', '+1 (617) 497-5555', '6179121234', '+1 617 354 7644', '+16179451548', '+1-617-492-7021', '+1 617 3540950', '+1 ( 617 ) 764 - 5693', '+1 (617) 772-5800', '+1 617 868-7711', '(617) 451-2395', '+1 (617) 574-7100', '+1 617 492 0070', '+1 617 876 5550', '+1 617 357-3000', '+1 781 3954899', '+1 617 242 9728', '781-641-5987', '617-876-1262', '+1 617 996 6100', '617-338-6800', '617-418-2200', '617-523-7577', '617-250-8433', '(617) 577-8856', '1-617-494-5300', '+1-617-242-7275', '617-666-3311', '+1 617 547 7400', '+1 617 868 8838', '+1 (617) 284-6212', '781-393-2343', '+16173494758', '+1-617-254-9600', '+1-617-547-3144', '(617) 864-3191', '+1 617 787 1080', '+16175779100', '+16173545065', '+16174924922', '+16174412101', '617-635-8399', '617-492-7083', '(617) 734-7700', '+16179452708', '617-227-0236', '+1-617-945-1768', '(617) 576-2220', '+1 617 616 5561', '617-394-2410', '617-547-0101', '617-267-1490', '+1 617 354 5279', '+1-617-783-9800', '+1-617-262-9600', '6177428565', '+1-617-439-9020', '+1 617 542-8623', '+1-617-496-1027', 'yes', '617-876-6068', '617-625-6600', '+1 617-208-6922;+1 617-208-6928', '+1 617 426.2000', '+1 617 623 0803', '617-635-8532', '617-575-2824', '+1 617 229 7900', '+16172531000', '617-871-2098', '+1 617 252 0005', '+16178646885', '617-354-0047', '+1 617 354 5471', '617-635-8436', '617-635-8534', '617-262-9562', '617.354.4200', '617-576-1900', '6175369000', '617 625 0001', '+1-617-782-7870', '+1 617 354 8881', '+1 617 437 0300', '617-635-7945', '+16175722000', '617-497-7771', '(617) 423-4630', '6179457030', '6172546030', '+1-617-519-9296', '617 262 6505', '+1-617-253-4680', '+1 617 354-4110', '617-354-5287', '+16179950900', '617-635-9989', '+16173496575', '+1 617 661-0969', '617-649-8600', '617-494-9330,  Forest City Management', '617-547-1950', '+1  617 422 0045', '+1 617 497-1136', '+1 617 421 8692', '1-617-421-9595', '617-484-2400', '+1 (617) 284-6228', '+1 617 6815000', '617-349-6530', '+1 617 547-0680', '+1 617 492 1234', '617-625-3459', '+1 617 381 4469', '617.954.2098', '+1 617 776-2100', '6175760280', '617-232-0300', '(617) 963-0889', '+ 1 617 242 9000', '617 330-1002', '617-266-8427', '+1-617-787-0767', '617-266-7171', '617-576-6163', '+1 617 627-9801', '+1 617.933.5000', '617-499-1451', '+1 617 547 3272', '+1-617-431-1433', '+1 617 354 8912', '+1 617 491 0843', '+1-617-868-0772', '617-876-2200', '617-635-9976', '617-635-9873', '+1 617 776-1234', '+1 617 666-2770', '+16176613040', '+1617958DELI', '+1 617 576 0108', '617-484-6668', '617-254-3110', '+1 617 354 3062', '+16175473721', '+1 617 227-8600', '+1 617 547 0120', '6177205544', '+1 617 666 8282', '617-266-7525', '+1-617-627-2000', '+1 617 491 6969', '+16179451008', '+1 857 209 2747', '781-646-6835', '+1 617 666-6072', '+16174211200', '+617 242 2229', '+1 (617) 629-0264', '+16174977626', '617-497-8454', '1-781-643-1377', '+1 617 262-8222', '617-225-2525', '617-349-6282', '6172244000', '+16176614900', '617-926-6979', '617-625-0600', '+1-617-248-8971', '617-238-5110', '+1-617-437-8884', '+16177786841', '617-349-6577', '617-536-5984', '+1 617 666 0000', '617.588.2628', '+16176282379', '617-227-6005', '6177445290'])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "filename = 'Boston.osm'\n",
    "                \n",
    "        \n",
    "def read_file():\n",
    "    for _, element in ET.iterparse(filename): \n",
    "        for child in element.getchildren(): \n",
    "            if child.tag=='tag': \n",
    "                key=child.get('k')\n",
    "                if 'phone' in key: \n",
    "                    phone_number = child.get('v')\n",
    "                    phone_set.add(phone_number)\n",
    "    return phone_set\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    street_types = defaultdict(set)\n",
    "    phone_set = set()\n",
    "    read_key = read_file()\n",
    "    print len(read_key)\n",
    "    print read_key\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Careful Review of the Phone Data did not indicate a lot of anomalies, however it does indicate varying formats as described below  \n",
    "* A few phone numbers have Country Code (+1) in them. E.g. +16175722000\n",
    "* A few phone numbers have spaces in between them instead of the hyphens. E.g. 617 242 9000\n",
    "* A few phone numbers have parenthesis. E.g. (857) 417-2396\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing\n",
    "## Cleansing Street Names\n",
    "\n",
    "This step involves creation of logic to cleanse the anomalies identified in the previous step. This steps involves a mapping dictionary which has the mapping between the incorrect format and the correct format of the street names in the form of key/value pairs. The psuedocode involves utilizing some of old code, that involved building a dictionary, by grouping street names that ended with a similar value. \n",
    "\n",
    "The cleansing step involves looping through the dictionary of elements and validating to see if the value can be found in the mapping dictionary. If the mapping is found then appropriate replacement is done to the street name using the python replace method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walnut St, ==> Walnut Street\n",
      "Pearl St. ==> Pearl Street\n",
      "Banks St. ==> Banks Street\n",
      "Marshall St. ==> Marshall Street\n",
      "Prospect St. ==> Prospect Street\n",
      "Main St. ==> Main Street\n",
      "Albion St. ==> Albion Street\n",
      "Saint Mary's St. ==> Saint Mary's Street\n",
      "Boylston St. ==> Boylston Street\n",
      "Stuart St. ==> Stuart Street\n",
      "Elm St. ==> Elm Street\n",
      "Newton ST ==> Newton Street\n",
      "Brighton Ave. ==> Brighton Avenue\n",
      "Spaulding Ave. ==> Spaulding Avenue\n",
      "Massachusetts Ave. ==> Massachusetts Avenue\n",
      "Somerville Ave. ==> Somerville Avenue\n",
      "Brentwood St ==> Brentwood Street\n",
      "Athol St ==> Athol Street\n",
      "Everett St ==> Everett Street\n",
      "South Waverly St ==> South Waverly Street\n",
      "Litchfield St ==> Litchfield Street\n",
      "Hampshire St ==> Hampshire Street\n",
      "Main St ==> Main Street\n",
      "Cambridge St ==> Cambridge Street\n",
      "Arsenal St ==> Arsenal Street\n",
      "Merrill St ==> Merrill Street\n",
      "Antwerp St ==> Antwerp Street\n",
      "1629 Cambridge St ==> 1629 Cambridge Street\n",
      "Elm St ==> Elm Street\n",
      "Lothrop St ==> Lothrop Street\n",
      "Charles St ==> Charles Street\n",
      "Dane St ==> Dane Street\n",
      "Norfolk St ==> Norfolk Street\n",
      "Bagnal St ==> Bagnal Street\n",
      "Cummington St ==> Cummington Street\n",
      "Holton St ==> Holton Street\n",
      "Mackin St ==> Mackin Street\n",
      "Waverly St ==> Waverly Street\n",
      "Mt Auburn St ==> Mt Auburn Street\n",
      "Duval St ==> Duval Street\n",
      "Kirkland St ==> Kirkland Street\n",
      "N Beacon St ==> N Beacon Street\n",
      "Leighton St ==> Leighton Street\n",
      "Richardson St ==> Richardson Street\n",
      "Winter St ==> Winter Street\n",
      "Ware St ==> Ware Street\n",
      "Portsmouth St ==> Portsmouth Street\n",
      "738 Commonwealth Ave ==> 738 Commonwealth Avenue\n",
      "Somerville Ave ==> Somerville Avenue\n",
      "Massachusetts Ave ==> Massachusetts Avenue\n",
      "Commonwealth Ave ==> Commonwealth Avenue\n",
      "Highland Ave ==> Highland Avenue\n",
      "Francesca Ave ==> Francesca Avenue\n",
      "Josephine Ave ==> Josephine Avenue\n",
      "Mystic Ave ==> Mystic Avenue\n",
      "Lexington Ave ==> Lexington Avenue\n",
      "Concord Ave ==> Concord Avenue\n",
      "Western Ave ==> Western Avenue\n",
      "Willow Ave ==> Willow Avenue\n",
      "College Ave ==> College Avenue\n",
      "Morrison Ave ==> Morrison Avenue\n",
      "Boston Ave ==> Boston Avenue\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Heights\", \"North\", \"East\", \"West\", \"South\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\", \n",
    "            \"Ave\": \"Avenue\", \n",
    "            \"Ave.\": \"Avenue\", \n",
    "            \"St\": \"Street\", \n",
    "            \"St,\": \"Street\", \n",
    "           \"St.\": \"Street\", \n",
    "           \"ST\": \"Street\"\n",
    "            }\n",
    "\n",
    "\n",
    "filename = 'boston.osm'\n",
    "                \n",
    "def process_addressanomalies(addressvalue): \n",
    "    match = street_type_re.search(addressvalue)\n",
    "    if match: \n",
    "        street_type = match.group()\n",
    "        if street_type not in expected: \n",
    "            street_types[street_type].add(addressvalue)\n",
    "\n",
    "def update_name(): \n",
    "    for k, v in read_key.iteritems(): \n",
    "        for vitem in v: \n",
    "            match = street_type_re.search(vitem) \n",
    "            val =  match.group(0)\n",
    "            if val in mapping: \n",
    "                new_name = vitem.replace(match.group(0), mapping[match.group(0)])\n",
    "                print vitem, \"==>\", new_name\n",
    "            \n",
    "\n",
    "def read_file():\n",
    "    for _, element in ET.iterparse(filename): \n",
    "        if element.tag == 'tag': \n",
    "            key=element.get('k')\n",
    "            if 'addr:street' in key:\n",
    "                addr_value = element.get('v')\n",
    "                process_addressanomalies(addr_value)\n",
    "    return street_types                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    street_types = defaultdict(set)\n",
    "    read_key = read_file()\n",
    "    better_adress = update_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing\n",
    "## Cleansing Phone Numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone Number Cleansing High Level Stats\n",
      "-------------------------------------------------\n",
      "Total Number of Phone Numbers to be Validated 475\n",
      "Total Number Cleaned Up Phone Numbers:  0\n",
      "Total Number of Phone Numbers to be cleaned Manually:  6\n",
      "List of Phone Numbers to be Cleaned Manually\n",
      "['To be Review Manually:yes',\n",
      " 'To be Review Manually:yes',\n",
      " 'To be Review Manually:AstraZeneca Neuroscience: T: (617) 679-1680',\n",
      " 'To be Review Manually:617-494-9330,  Forest City Management',\n",
      " 'To be Review Manually:Phone 617.714.0555',\n",
      " 'To be Review Manually:+1617958DELI']\n",
      "-------------------------------------------------\n",
      "Sample List of Cleansed Phone Numbers\n",
      "617-635-8532 ---> 6176358532\n",
      "617-254-8383 ---> 6172548383\n",
      "617-349-6555 ---> 6173496555\n",
      "617-266-8427 ---> 6172668427\n",
      "781-393-2333 ---> 7813932333\n",
      "617-284-7800 ---> 6172847800\n",
      "617-635-8497 ---> 6176358497\n",
      "617-635-9976 ---> 6176359976\n",
      "617-354-0047 ---> 6173540047\n",
      "617-666-3311 ---> 6176663311\n",
      "617-262-9562 ---> 6172629562\n",
      "List of PhoneNumbers to be Cleansed Manually\n",
      "['To be Review Manually:yes',\n",
      " 'To be Review Manually:yes',\n",
      " 'To be Review Manually:AstraZeneca Neuroscience: T: (617) 679-1680',\n",
      " 'To be Review Manually:617-494-9330,  Forest City Management',\n",
      " 'To be Review Manually:Phone 617.714.0555',\n",
      " 'To be Review Manually:+1617958DELI',\n",
      " 'To be Review Manually:6172086922;6172086928',\n",
      " 'To be Review Manually:26777722147']\n"
     ]
    }
   ],
   "source": [
    "# Importing all the Needed Libraries\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "'''Regular Expressions for different kinds of Cleaning'''\n",
    "# Regular Expression to check whether the phone number is starting with + 1 \n",
    "phonecheckone = re.compile(r'^\\+[1]\\s*')\n",
    "\n",
    "# Regular Expression to check whether the phone number has a hyphen in it \n",
    "phonecheckhyphen = re.compile(r'\\-+')\n",
    "\n",
    "# Regular Expression to check whether the phone number has any white spaces in it \n",
    "phonecheckspace = re.compile(r'\\s+?')\n",
    "\n",
    "# Regular Expression to check whether the phone number contains any parenthesis\n",
    "phonecheckpar1 = re.compile(r'\\(+')\n",
    "phonecheckpar2 = re.compile(r'\\)+')\n",
    "\n",
    "# Regular Expression to check whether the phone number contains any periods\n",
    "phonecheckperiod = re.compile(r'\\.+')\n",
    "\n",
    "# Regular Expression to check whether the phone number contains any alphabets in them \n",
    "phonecheckalpha = re.compile(r'[a-zA-Z].*')\n",
    "\n",
    "# Regular Expression to check whether the phone number has any commas in it \n",
    "phonecheckcomma = re.compile(r'\\,+')\n",
    "\n",
    "filename = 'Boston.osm'\n",
    "\n",
    "\n",
    "\n",
    "'''Function that does the phone number cleanse through an iterative process'''\n",
    "def clean_phone(): \n",
    "    \n",
    "    '''For loop to check for alphabets in the phonenumber. Phone Numbers that have alphabets\n",
    "    are populated in a seperate list for manual review. Phone numbers that do not have any alphabets proceed along\n",
    "    to other steps in the cleaning process '''\n",
    "    for phone_item in read_key:\n",
    "        match=phonecheckalpha.search(phone_item)\n",
    "        if match: \n",
    "            phone_list1Manual.append(\"To be Review Manually:\" + phone_item)\n",
    "        else:\n",
    "            phone_list1.append(phone_item)\n",
    "\n",
    "    '''For Loop to cleanse any whitespaces in the phonenumber'''\n",
    "    for phonelist1_item in phone_list1: \n",
    "        match=phonecheckspace.search(phonelist1_item)\n",
    "        if match: \n",
    "            phonelist1_itemnew=phonelist1_item.replace(match.group(0), '')\n",
    "            phone_list2.append(phonelist1_itemnew)\n",
    "        else: \n",
    "            phone_list2.append(phonelist1_item)\n",
    "    \n",
    "    '''For Loop to cleanse any Hyphens in the phonenumber'''\n",
    "    for phonelist2_item in phone_list2: \n",
    "        match = phonecheckhyphen.search(phonelist2_item)\n",
    "        if match: \n",
    "            phonelist2_itemnew = phonelist2_item.replace(match.group(0), '')\n",
    "            phone_list3.append(phonelist2_itemnew)\n",
    "        else: \n",
    "            phone_list3.append(phonelist2_item)\n",
    "    \n",
    "    '''For Loop to cleanse any phonenumbers that start with +1 '''  \n",
    "    for phonelist3_item in phone_list3: \n",
    "        match = phonecheckone.search(phonelist3_item)\n",
    "        if match: \n",
    "            phonelist3_itemnew = phonelist3_item.replace(match.group(0), '')\n",
    "            phone_list4.append(phonelist3_itemnew)\n",
    "        else: \n",
    "            phone_list4.append(phonelist3_item)\n",
    "    \n",
    "    '''For loop to cleanse any paranthesis in the phonenumber'''\n",
    "    for phonelist4_item in phone_list4: \n",
    "        match = phonecheckpar1.search(phonelist4_item)\n",
    "        if match: \n",
    "            phonelist4_itemnew = phonelist4_item.replace(match.group(0), '')\n",
    "            phone_list5.append(phonelist4_itemnew)\n",
    "        else: \n",
    "            phone_list5.append(phonelist4_item)\n",
    "\n",
    "    \n",
    "    for phonelist5_item in phone_list5: \n",
    "        match = phonecheckpar2.search(phonelist5_item)\n",
    "        if match: \n",
    "            phonelist5_itemnew = phonelist5_item.replace(match.group(0), '')\n",
    "            phone_list6.append(phonelist5_itemnew)\n",
    "        else: \n",
    "            phone_list6.append(phonelist5_item)\n",
    "    \n",
    "    '''For loop to cleanse any periods in the phonenumber'''\n",
    "    for phonelist6_item in phone_list6: \n",
    "        match = phonecheckperiod.search(phonelist6_item)\n",
    "        if match: \n",
    "            phonelist6_itemnew = phonelist6_item.replace(match.group(0), '')\n",
    "            phone_list7.append(phonelist6_itemnew)\n",
    "        else: \n",
    "            phone_list7.append(phonelist6_item)\n",
    "\n",
    "    '''For loop to cleanse any phone numbers that begin with a 1'''\n",
    "    for phonelist7_item in phone_list7: \n",
    "        if phonelist7_item.startswith('1') or phonelist7_item.startswith('+') : \n",
    "            phone_list8.append(phonelist7_item[1:])\n",
    "        else: \n",
    "            phone_list8.append(phonelist7_item)\n",
    "\n",
    "    '''For loop to cleanse any commas in the phonenumbers'''\n",
    "    for phonelist8_item in phone_list8: \n",
    "        match = phonecheckcomma.search(phonelist8_item)\n",
    "        if match: \n",
    "            phonelist8_itemnew = phonelist8_item.replace(match.group(0), '')\n",
    "            phone_list9.append(phonelist8_itemnew)\n",
    "        else: \n",
    "            phone_list9.append(phonelist8_item)\n",
    "        \n",
    "def checkalphabetsfromsource(): \n",
    "     for phonevalue in read_key:\n",
    "        match=phonecheckalpha.search(phonevalue)\n",
    "        if match: \n",
    "            phone = phonevalue.replace(match.group(0), '')\n",
    "            phoneset_alpha.append(phonevalue)\n",
    "        else: \n",
    "            phoneset_noalpha.append(phonevalue)\n",
    "\n",
    "    \n",
    "       \n",
    "def read_file():\n",
    "    for _, element in ET.iterparse(filename): \n",
    "        for child in element.getchildren(): \n",
    "            if child.tag=='tag': \n",
    "                key=child.get('k')\n",
    "                if 'phone' in key: \n",
    "                    phone_number = child.get('v')\n",
    "                    phone_set.append(phone_number)\n",
    "    return phone_set\n",
    "                \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    count = 0 \n",
    "    \n",
    "    '''Initializing the different list before using them through the \n",
    "    different steps of the cleanse process with the regular expression'''\n",
    "    \n",
    "    \n",
    "    phone_set = []\n",
    "    phoneset_alpha=[]\n",
    "    phoneset_noalpha=[]\n",
    "    phone_list1=[]\n",
    "    phone_list1Manual=[]\n",
    "    phone_list2=[]\n",
    "    phone_list3=[]\n",
    "    phone_list4=[]\n",
    "    phone_list5=[]\n",
    "    phone_list6=[]\n",
    "    phone_list7=[]\n",
    "    phone_list8=[]\n",
    "    phone_list9=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    read_key = read_file()\n",
    "    clean_phone = clean_phone()\n",
    "    check_alphabets = checkalphabetsfromsource()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print \"Phone Number Cleansing High Level Stats\"\n",
    "    print \"-------------------------------------------------\"\n",
    "    print \"Total Number of Phone Numbers to be Validated\", len(read_key)\n",
    "    print \"Total Number Cleaned Up Phone Numbers: \", count \n",
    "    print \"Total Number of Phone Numbers to be cleaned Manually: \", len(phone_list1Manual) \n",
    "    print \"List of Phone Numbers to be Cleaned Manually\" \n",
    "    pprint.pprint(phone_list1Manual)\n",
    "    print \"-------------------------------------------------\"\n",
    "    \n",
    "    \n",
    "    '''Sample List of Cleansed Phone Numbers'''\n",
    "    print \"Sample List of Cleansed Phone Numbers\"\n",
    "    for i in range(len(phone_list1)): \n",
    "        if len(phone_list9[i])==10 and count<=10: \n",
    "            print phone_list1[i],    \"--->\" ,  phone_list9[i]\n",
    "            count = count + 1 \n",
    "            \n",
    "        elif len(phone_list9[i])>10: \n",
    "            phone_list1Manual.append(\"To be Review Manually:\" + phone_list9[i])\n",
    "        \n",
    "        else: \n",
    "            continue\n",
    "    '''List of PhoneNumbers to be Cleansed Manually'''\n",
    "    print \"List of PhoneNumbers to be Cleansed Manually\"\n",
    "    pprint.pprint(phone_list1Manual)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing\n",
    "## Cleansing Zip Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip Code Cleansing High Level Stats\n",
      "---------------------------------------------------------------------\n",
      "Total Number of Zip Codes Encountered in the File:  1685\n",
      "Total Number of Zip Codes to be Cleaned Manually:  5\n",
      "Total Number of Zip Codes Cleaned Using Regular Expressions:  1680\n",
      "Total Number of Zip Codes that were Cleaned:  1680\n",
      "---------------------------------------------------------------------\n",
      "Sample List of Cleansed ZipCodes\n",
      "Old Post Code: 02114-3203--->Cleaned Post Code: 02114\n",
      "Old Post Code: 02110-1301--->Cleaned Post Code: 02110\n",
      "Old Post Code: 02140-1340--->Cleaned Post Code: 02140\n",
      "Old Post Code: 02284-6028--->Cleaned Post Code: 02284\n",
      "Old Post Code: 02134-1409--->Cleaned Post Code: 02134\n",
      "Old Post Code: 02138-2706--->Cleaned Post Code: 02138\n",
      "Old Post Code: 02114-3203--->Cleaned Post Code: 02114\n",
      "Old Post Code: 02138-1901--->Cleaned Post Code: 02138\n",
      "Old Post Code: 02138-3824--->Cleaned Post Code: 02138\n",
      "Old Post Code: 02138-2701--->Cleaned Post Code: 02138\n",
      "Old Post Code: 02138-2903--->Cleaned Post Code: 02138\n",
      "---------------------------------------------------------------------\n",
      "List of Zipcodes to be Cleansed Manually\n",
      "['To be Review Manually:MA',\n",
      " 'To be Review Manually:MA 02116',\n",
      " 'To be Review Manually:MA 02116',\n",
      " 'To be Review Manually:MA',\n",
      " 'To be Review Manually:MA 02116']\n"
     ]
    }
   ],
   "source": [
    "# Importing all the Needed Libraries\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Regular Expression to check whether the phone number contains any alphabets in them \n",
    "zipcheckalpha = re.compile(r'[a-zA-Z].*')\n",
    "zipcheckhyphen = re.compile(r'^(\\d{5})-\\d{4}$')\n",
    "\n",
    "def identifypostcode(): \n",
    "    \n",
    "    '''Identifying Post Codes that have Characters \n",
    "        in them and sending them in a manual review list'''\n",
    "    \n",
    "    for zip_item in read_key:\n",
    "        match=zipcheckalpha.search(zip_item)\n",
    "        if match: \n",
    "            zipmanual.append(\"To be Review Manually:\" + zip_item)\n",
    "        else:\n",
    "            zipauto.append(zip_item)\n",
    "\n",
    "def fixpostcode(): \n",
    "    \n",
    "    '''Check for Hyphens in Post Code and Extract the first part of the Zipcode''' \n",
    "    \n",
    "    for zipcode_item in zipauto: \n",
    "        match = zipcheckhyphen.search(zipcode_item)\n",
    "        if match: \n",
    "            ziphyphen.append(zipcode_item)\n",
    "            zipcode_split = zipcode_item.split('-')\n",
    "            ziphyphencleaned.append(zipcode_split[0]) \n",
    "        else: \n",
    "            ziphyphencleaned.append(zipcode_item)\n",
    "        \n",
    "\n",
    "            \n",
    "def read_file():\n",
    "    for _, element in ET.iterparse(filename): \n",
    "        for child in element.getchildren(): \n",
    "            if child.tag=='tag': \n",
    "                key=child.get('k')\n",
    "                if 'addr:postcode' in key: \n",
    "                    post_code = child.get('v')\n",
    "                    postcode.append(post_code)\n",
    "    return postcode\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    count = 0 \n",
    "    \n",
    "    '''Initializing the different list before using them through the \n",
    "    different steps of the cleanse process with the regular expression'''\n",
    "    \n",
    "    \n",
    "    postcode = []\n",
    "    zipmanual=[]\n",
    "    ziphyphen=[]\n",
    "    ziphyphencleaned=[]\n",
    "    zipauto=[]\n",
    "    \n",
    "    read_key = read_file()\n",
    "    identifypostcode = identifypostcode()\n",
    "    fixpostcode=fixpostcode()\n",
    "    \n",
    "    print \"Zip Code Cleansing High Level Stats\"\n",
    "    print \"---------------------------------------------------------------------\"\n",
    "    print \"Total Number of Zip Codes Encountered in the File: \", len(read_key)\n",
    "    print \"Total Number of Zip Codes to be Cleaned Manually: \", len(zipmanual)\n",
    "    print \"Total Number of Zip Codes Cleaned Using Regular Expressions: \", len(zipauto)\n",
    "    print \"Total Number of Zip Codes that were Cleaned: \", len(ziphyphencleaned)\n",
    "    print \"---------------------------------------------------------------------\"\n",
    "\n",
    "    '''Sample List of Cleansed ZipCodes'''\n",
    "    \n",
    "    print \"Sample List of Cleansed ZipCodes\"\n",
    "    for i in range(len(ziphyphencleaned)): \n",
    "        if len(zipauto[i])>5 and count<=10: \n",
    "            print \"Old Post Code: \" + zipauto[i] + \"--->\" + \"Cleaned Post Code: \" + ziphyphencleaned[i]\n",
    "            count = count + 1 \n",
    "        else: \n",
    "            continue\n",
    "    \n",
    "    print \"---------------------------------------------------------------------\"\n",
    "            \n",
    "    '''List of Zipcodes to be Cleansed Manually'''\n",
    "    print \"List of Zipcodes to be Cleansed Manually\"\n",
    "    pprint.pprint(zipmanual)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XML to a JSON Conversion \n",
    "## This step involves parsing through the XML file to create a JSON file, which will then be used to import into mongoDB. We need to follow the below rules for translation \n",
    "\n",
    "* Process only 2 types of top level tags: \"node\" and \"way\"\n",
    "* All attributes of \"node\" and \"way\" should be turned into regular key/value pairs, except: attributes in the CREATED array should be added under a key \"created\", attributes for latitude and longitude should be added to a \"pos\" array, for use in geospacial indexing. Make sure the values inside \"pos\" array are floats and not strings.\n",
    "* If second level tag \"k\" value contains problematic characters, it should be ignored\n",
    "* If second level tag \"k\" value starts with \"addr:\", it should be added to a dictionary \"address\"\n",
    "* If second level tag \"k\" value does not start with \"addr:\", but contains \":\", you can process it same as any other tag.\n",
    "* If there is a second \":\" that separates the type/direction of a street, the tag should be ignored\n",
    "\n",
    "\n",
    "**In addition as a part of the import the street names, phone numbers and zipcodes were also cleaned up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "from collections import defaultdict\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "'''Parameters for Zipcode Clean Up'''\n",
    "\n",
    "zipcheckalpha = re.compile(r'[a-zA-Z].*')\n",
    "zipcheckhyphen = re.compile(r'^(\\d{5})-\\d{4}$')\n",
    "\n",
    "\n",
    "'''Parameters for Address Street Name Clean Up'''\n",
    "\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Heights\", \"North\", \"East\", \"West\", \"South\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\": \"Road\", \n",
    "            \"Ave\": \"Avenue\", \n",
    "            \"Ave.\": \"Avenue\", \n",
    "            \"St\": \"Street\", \n",
    "            \"St,\": \"Street\", \n",
    "           \"St.\": \"Street\", \n",
    "           \"ST\": \"Street\"\n",
    "            }\n",
    "\n",
    "\n",
    "'''Parameters for Phone Clean Up'''\n",
    "\n",
    "# Regular Expression to check whether the phone number is starting with + 1 \n",
    "phonecheckone = re.compile(r'^\\+[1]\\s*')\n",
    "\n",
    "# Regular Expression to check whether the phone number has a hyphen in it \n",
    "phonecheckhyphen = re.compile(r'\\-+')\n",
    "\n",
    "# Regular Expression to check whether the phone number has any white spaces in it \n",
    "phonecheckspace = re.compile(r'\\s+?')\n",
    "\n",
    "# Regular Expression to check whether the phone number contains any parenthesis\n",
    "phonecheckpar1 = re.compile(r'\\(+')\n",
    "phonecheckpar2 = re.compile(r'\\)+')\n",
    "\n",
    "# Regular Expression to check whether the phone number contains any periods\n",
    "phonecheckperiod = re.compile(r'\\.+')\n",
    "\n",
    "# Regular Expression to check whether the phone number contains any alphabets in them \n",
    "phonecheckalpha = re.compile(r'[a-zA-Z].*')\n",
    "\n",
    "# Regular Expression to check whether the phone number has any commas in it \n",
    "phonecheckcomma = re.compile(r'\\,+')\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "'''Cleansing Function to Clean Zipcodes'''\n",
    "\n",
    "def clean_zipcode(zipcode):\n",
    "    ca = clean_zalpha(zipcode)\n",
    "    if ca:\n",
    "        return zipcode \n",
    "    else: \n",
    "        cleaned_zipcode = clean_zhyphen(zipcode)\n",
    "        return cleaned_zipcode \n",
    "\n",
    "\n",
    "def clean_zalpha(zipcode): \n",
    "    match = zipcheckalpha.search(zipcode)\n",
    "    if match: \n",
    "        return zipcode \n",
    "    else: \n",
    "        pass\n",
    "            \n",
    "\n",
    "def clean_zhyphen(zipcode): \n",
    "    match = phonecheckhyphen.search(zipcode)\n",
    "    if match: \n",
    "        zipcode = match.group(0).split('-')[0]\n",
    "        return zipcode\n",
    "    else: \n",
    "        return zipcode\n",
    "\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "'''Cleansing Function to Clean Street Names'''\n",
    "\n",
    "def clean_streetname(old_name): \n",
    "    match = street_type_re.search(old_name) \n",
    "    val =  match.group(0)\n",
    "    if val in mapping: \n",
    "        name = old_name.replace(match.group(0), mapping[match.group(0)])\n",
    "        return name\n",
    "    else:\n",
    "        name = old_name\n",
    "        return name\n",
    "#-------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "'''Cleansing Function to Clean Phone Numbers'''\n",
    "\n",
    "def clean_phone(phone):\n",
    "    ca = clean_alpha(phone)\n",
    "    if ca:\n",
    "        return phone \n",
    "    else: \n",
    "        whitespace= clean_whitespaces(phone) \n",
    "        hyphen= clean_hyphen(whitespace)\n",
    "        plusone=clean_plusone(hyphen)\n",
    "        paren1=clean_paranthesis1(plusone)\n",
    "        paren2= clean_paranthesis2(paren1)\n",
    "        prd= clean_period(paren2)\n",
    "        cleanone = clean_one(prd)\n",
    "        cleancomma = clean_comma(cleanone)\n",
    "        return cleancomma \n",
    "\n",
    "\n",
    "def clean_alpha(phone): \n",
    "    match = phonecheckalpha.search(phone)\n",
    "    if match: \n",
    "        return phone \n",
    "    else: \n",
    "        pass\n",
    "            \n",
    "def clean_whitespaces(phone): \n",
    "    match=phonecheckspace.search(phone)\n",
    "    if match: \n",
    "        phone=phone.replace(match.group(0), '')\n",
    "        return phone\n",
    "    else: \n",
    "        return phone\n",
    "\n",
    "\n",
    "def clean_hyphen(phone): \n",
    "    match = phonecheckhyphen.search(phone)\n",
    "    if match: \n",
    "        phone = phone.replace(match.group(0), '')\n",
    "        return phone\n",
    "    else: \n",
    "        return phone\n",
    "        \n",
    "def clean_plusone(phone): \n",
    "    match = phonecheckone.search(phone)\n",
    "    if match: \n",
    "        phone = phone.replace(match.group(0), '')\n",
    "        return phone\n",
    "    else: \n",
    "        return phone \n",
    "        \n",
    "def clean_paranthesis1(phone): \n",
    "    match = phonecheckpar1.search(phone)\n",
    "    if match: \n",
    "        phone = phone.replace(match.group(0), '')\n",
    "        return phone\n",
    "    else: \n",
    "        return phone\n",
    "\n",
    "def clean_paranthesis2(phone): \n",
    "    match = phonecheckpar2.search(phone)\n",
    "    if match: \n",
    "        phone = phone.replace(match.group(0), '')\n",
    "        return phone\n",
    "    else: \n",
    "        return phone\n",
    "\n",
    "        \n",
    "def clean_period(phone): \n",
    "    match = phonecheckperiod.search(phone)\n",
    "    if match: \n",
    "        phone = phone.replace(match.group(0), '')\n",
    "        return phone\n",
    "    else: \n",
    "        return phone\n",
    "        \n",
    "def clean_one(phone): \n",
    "    if phone.startswith('1') or phone.startswith('+'): \n",
    "        phone = phone[1:]\n",
    "        return phone \n",
    "    else: \n",
    "        return phone \n",
    "\n",
    "\n",
    "def clean_comma(phone): \n",
    "    match = phonecheckcomma.search(phone)\n",
    "    if match: \n",
    "        phone = phone.replace(match.group(0), '')\n",
    "        return phone\n",
    "    else: \n",
    "        return phone       \n",
    "    \n",
    "#-------------------------------------------------------------------------------------------\n",
    "    \n",
    "def floatOrNofloat(n):\n",
    "    return float(n) if n else None\n",
    "    \n",
    "def shape_element(element): \n",
    "    node = defaultdict(dict) \n",
    "    if element.tag == \"node\" or element.tag == \"way\":\n",
    "        \n",
    "        node[\"tag\"] = element.tag\n",
    "\n",
    "        node [\"id\"] = element.get('id')\n",
    "\n",
    "        lat = element.get('lat')\n",
    "\n",
    "        lon = element.get('lon')\n",
    "\n",
    "        if lat or lon:\n",
    "            node['pos'] = [floatOrNofloat(lat), floatOrNofloat(lon)]\n",
    "        \n",
    "        node[\"created\"] = {}\n",
    "\n",
    "        for key in CREATED:\n",
    "            node[\"created\"][key] = element.get(key)\n",
    "        \n",
    "        for child in element.getchildren():\n",
    "            \n",
    "            key = child.get(\"k\")\n",
    "            ref = child.get(\"ref\")\n",
    "            \n",
    "            if key == 'address': \n",
    "                node['fulladdress'] = child.get('v')\n",
    "            \n",
    "            '''Included the logic to clean the Phone Number by calling the Phone Cleaning Function prior to JSON Import'''\n",
    "            if key == 'phone': \n",
    "                if len(clean_phone(child.get('v'))) == 10: \n",
    "                    node['phonenumber'] = clean_phone(child.get('v'))\n",
    "                else: \n",
    "                    node['phonenumber'] = 'Phone Number Removed Due to Incorrect Value'\n",
    "                    \n",
    "            \n",
    "            if key is not None: \n",
    "                if key.startswith('addr:'):\n",
    "                    split_key = key.split(\":\")\n",
    "                    \n",
    "                    '''Included the logic to clean the Street Name and Post Codes by calling the respective functions\n",
    "                    prior to JSON Import'''\n",
    "                    \n",
    "                    if split_key[1] == 'street': \n",
    "                        node['address'][split_key[1]]= clean_streetname(child.get('v'))\n",
    "                    elif split_key[1] == 'postcode':\n",
    "                        node['address'][split_key[1]]= clean_zipcode(child.get('v'))\n",
    "                    else: \n",
    "                        node['address'][split_key[1]] = child.get('v')                        \n",
    "\n",
    "                elif 'amenity' in key: \n",
    "                    node['amenity'] = child.get('v')\n",
    "                elif 'name' in key: \n",
    "                    node['name'] = child.get('v')\n",
    "            \n",
    "            if ref: \n",
    "                if \"node_refs\" not in node: \n",
    "                    node[\"node_refs\"] = []\n",
    "                else: \n",
    "                    node[\"node_refs\"].append(ref)\n",
    "        \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "\n",
    "    \n",
    "def process_map(file_in, pretty = False):\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "def test():\n",
    "    data = process_map('Boston.osm', True)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the step above is the creation of **\"Boston.osm.json\"** file, which is later been used to import into MongoDB. In addition as a part of the import the street names, phone numbers and zipcodes were also cleaned up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up for Mongo Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), u'boston')\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "client = MongoClient()\n",
    "db = client.boston\n",
    "print db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis/Data Exploration in MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Size of the Original OSM File and the JSON File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original OSM file is 100.298777 MB\n",
      "The JSON file is 145.96108 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print 'The original OSM file is {} MB'.format(os.path.getsize('Boston.osm')/1.0e6)\n",
    "print 'The JSON file is {} MB'.format(os.path.getsize('Boston.osm' + \".json\")/1.0e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = db['bostonc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520261"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Nodes and Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 444899\n",
      "Number of ways: 75362\n"
     ]
    }
   ],
   "source": [
    "print \"Number of nodes:\",boston.find({'tag': 'node'}).count()\n",
    "print \"Number of ways:\", boston.find({'tag': 'way'}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 Contributors along with the UserNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'crschmidt', u'count': 269155},\n",
      " {u'_id': u'jremillard-massgis', u'count': 64989},\n",
      " {u'_id': u'wambag', u'count': 29468},\n",
      " {u'_id': u'OceanVortex', u'count': 27793},\n",
      " {u'_id': u'ryebread', u'count': 21755},\n",
      " {u'_id': u'morganwahl', u'count': 20291},\n",
      " {u'_id': u'mapper999', u'count': 8309},\n",
      " {u'_id': u'cspanring', u'count': 6817},\n",
      " {u'_id': u'JasonWoof', u'count': 5439},\n",
      " {u'_id': u'synack', u'count': 5042}]\n"
     ]
    }
   ],
   "source": [
    "result = boston.aggregate( [\n",
    "                                        { \"$group\" : {\"_id\" : \"$created.user\", \"count\" : { \"$sum\" : 1} } },\n",
    "                                        { \"$sort\" : {\"count\" : -1} }, \n",
    "                                        { \"$limit\" : 10 } ] )\n",
    "\n",
    "pprint.pprint(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of Top 50 Amenities in the Boston Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'parking', u'count': 545},\n",
      " {u'_id': u'bench', u'count': 495},\n",
      " {u'_id': u'restaurant', u'count': 398},\n",
      " {u'_id': u'bicycle_parking', u'count': 214},\n",
      " {u'_id': u'school', u'count': 205},\n",
      " {u'_id': u'place_of_worship', u'count': 184},\n",
      " {u'_id': u'library', u'count': 162},\n",
      " {u'_id': u'cafe', u'count': 158},\n",
      " {u'_id': u'fast_food', u'count': 114},\n",
      " {u'_id': u'bicycle_rental', u'count': 89},\n",
      " {u'_id': u'university', u'count': 77},\n",
      " {u'_id': u'post_box', u'count': 69},\n",
      " {u'_id': u'bank', u'count': 65},\n",
      " {u'_id': u'waste_basket', u'count': 59},\n",
      " {u'_id': u'pub', u'count': 49},\n",
      " {u'_id': u'fuel', u'count': 41},\n",
      " {u'_id': u'fountain', u'count': 34},\n",
      " {u'_id': u'pharmacy', u'count': 34},\n",
      " {u'_id': u'atm', u'count': 33},\n",
      " {u'_id': u'hospital', u'count': 31},\n",
      " {u'_id': u'fire_station', u'count': 31},\n",
      " {u'_id': u'drinking_water', u'count': 31},\n",
      " {u'_id': u'car_sharing', u'count': 28},\n",
      " {u'_id': u'bar', u'count': 28},\n",
      " {u'_id': u'parking_space', u'count': 27},\n",
      " {u'_id': u'post_office', u'count': 24},\n",
      " {u'_id': u'theatre', u'count': 23},\n",
      " {u'_id': u'college', u'count': 22},\n",
      " {u'_id': u'bicycle_repair_station', u'count': 21},\n",
      " {u'_id': u'recycling', u'count': 16},\n",
      " {u'_id': u'police', u'count': 15},\n",
      " {u'_id': u'toilets', u'count': 14},\n",
      " {u'_id': u'telephone', u'count': 12},\n",
      " {u'_id': u'emergency_phone', u'count': 10},\n",
      " {u'_id': u'public_building', u'count': 9},\n",
      " {u'_id': u'dentist', u'count': 8},\n",
      " {u'_id': u'arts_centre', u'count': 8},\n",
      " {u'_id': u'parking_entrance', u'count': 7},\n",
      " {u'_id': u'cinema', u'count': 7},\n",
      " {u'_id': u'grave_yard', u'count': 7},\n",
      " {u'_id': u'townhall', u'count': 6},\n",
      " {u'_id': u'doctors', u'count': 5},\n",
      " {u'_id': u'social_facility', u'count': 5},\n",
      " {u'_id': u'car_rental', u'count': 5},\n",
      " {u'_id': u'marketplace', u'count': 5},\n",
      " {u'_id': u'vending_machine', u'count': 4},\n",
      " {u'_id': u'car_wash', u'count': 4},\n",
      " {u'_id': u'community_centre', u'count': 4},\n",
      " {u'_id': u'swimming_pool', u'count': 3},\n",
      " {u'_id': u'magazine_box', u'count': 3}]\n"
     ]
    }
   ],
   "source": [
    "result = boston.aggregate( [            {'$match': {'amenity': {'$exists': 1}}},\n",
    "                                        { \"$group\" : {\"_id\" : \"$amenity\", \"count\" : { \"$sum\" : 1} } },\n",
    "                                        { \"$sort\" : {\"count\" : -1} }, \n",
    "                                        { \"$limit\" : 50 } ] )\n",
    "\n",
    "pprint.pprint(list(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the List of Colleges from the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'College': u'North Bennet Street School', u'Name': 1},\n",
      " {u'College': u'Radcliffe Quad', u'Name': 1},\n",
      " {u'College': u'Bunker Hill Community College', u'Name': 1},\n",
      " {u'College': u'Emerson College', u'Name': 7},\n",
      " {u'College': u'Berklee College of Music', u'Name': 7},\n",
      " {u'College': u'Emerson College \\u2013 Walker Building', u'Name': 1},\n",
      " {u'College': u'Emerson College \\u2013 Tuffte Performing Arts Center',\n",
      "  u'Name': 1},\n",
      " {u'College': u'Fisher College', u'Name': 1},\n",
      " {u'College': u'Emerson College - Little Building', u'Name': 1},\n",
      " {u'College': u'Emerson College \\u2013 Piano Row', u'Name': 1}]\n"
     ]
    }
   ],
   "source": [
    "colleges = boston.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1},\n",
    "                                 \"amenity\":\"college\",}},      \n",
    "                      {\"$group\":{\"_id\":{\"Name\":\"$name\"},\n",
    "                                 \"count\":{\"$sum\":1}}},\n",
    "                      {\"$project\":{\"_id\":0,\n",
    "                                  \"College\":\"$_id.Name\",\n",
    "                                  \"Name\":\"$count\"}},\n",
    "                      {\"$sort\":{\"Count\":-1}}, \n",
    "                      {\"$limit\":10}])\n",
    "pprint.pprint(list(colleges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This list is definitely missing some of the key universities in the Boston Area like Harvard, MIT, NorthEastern. On further review of the dataset I noticed that the missing schools and colleges are infact a part of the dataset, they just don't have an amenity of college attached to them** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the list of Public Buildings in the Boston Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'Building': None, u'Name': 1},\n",
      " {u'Building': u'Social Security Administration', u'Name': 1},\n",
      " {u'Building': u'Suffolk', u'Name': 5},\n",
      " {u'Building': u'Middlesex', u'Name': 2}]\n"
     ]
    }
   ],
   "source": [
    "building = boston.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1},\n",
    "                                 \"amenity\":\"public_building\",}},      \n",
    "                      {\"$group\":{\"_id\":{\"Name\":\"$name\"},\n",
    "                                 \"count\":{\"$sum\":1}}},\n",
    "                      {\"$project\":{\"_id\":0,\n",
    "                                  \"Building\":\"$_id.Name\",\n",
    "                                  \"Name\":\"$count\"}},\n",
    "                      {\"$sort\":{\"Count\":-1}}, \n",
    "                      {\"$limit\":10}])\n",
    "pprint.pprint(list(building))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Top Cities in the Boston Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'Boston', u'count': 619},\n",
      " {u'_id': u'Cambridge', u'count': 555},\n",
      " {u'_id': u'Somerville', u'count': 240},\n",
      " {u'_id': u'Arlington', u'count': 172},\n",
      " {u'_id': u'Allston', u'count': 17},\n",
      " {u'_id': u'Arlington, MA', u'count': 9},\n",
      " {u'_id': u'Charlestown', u'count': 9},\n",
      " {u'_id': u'Watertown', u'count': 9},\n",
      " {u'_id': u'Cambridge, MA', u'count': 8},\n",
      " {u'_id': u'Brookline', u'count': 7}]\n"
     ]
    }
   ],
   "source": [
    "cities = boston.aggregate([\n",
    "        {\"$match\": {\"address.city\":{\"$exists\":1}}}, \n",
    "        {\"$group\":{\"_id\":\"$address.city\", \"count\":{\"$sum\":1}}},\n",
    "        {\"$sort\": {\"count\": -1}}, \n",
    "        {\"$limit\":10}                                 \n",
    "    ])\n",
    "\n",
    "pprint.pprint(list(cities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Other Suggested Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the size of the Boston Data Set that was analyzed and the number of issues that existed with the dataset, it was a lot better than I anticipated. That said, there are definitely areas for improvement. E.g.We noticed inconsistencies with the street names while executing our auditing in Python. We also noticed some minor anomalies with ZipCode and the Phone data. In addition we found other issues around missing data, or the data being associated with different types \n",
    "\n",
    "1. When we executed a query to extract the list of colleges in the Boston Area based on amenity == \"college\", the result set was missing some of the key institutions in the Boston area (e.g. MIT, Harvard, NorthEastern). On further analysis by looking at OSM file we noticed that the data is infact present, but just that it was associated with a different type. \n",
    "2. Similarly, when we executed a query to extract the list of public buildings in the Boston Area based on amenity == \"public_building, not a whole lot of buildings showed up. \n",
    "\n",
    "If we further analyze the root cause for 1 and 2, we can definitely conclude that these are the effects of manual contribution from hundreds and hundreds of users over the web. \n",
    "\n",
    "## Here are some recommendations to improve the quality of data within Open Street Maps\n",
    "### Option 1: Structured Input Form: \n",
    "\n",
    "One approach to rectify this would be to use a structured input form to consume data from the users \n",
    "\n",
    "#### Pros: \n",
    "1. Forces users to input data by adhering to a general format \n",
    "\n",
    "#### Cons: \n",
    "2. In some cases the form restrictions might prevent users from entering valid data. In situations like those users might either leave the data as blank and proceed or enter data to adhere to the form settings, but might be incorrect data. E.g. Say if the List of Cities is presented as a Drop Down Value, and the user is not able to find the relevant city, they might be inclined to select another neighbouring city for the purposes of ingesting the value into the maps. This would lead to other issues and subsequent clean ups. \n",
    "\n",
    "### Option 2: Address Validation using WebService Calls with other Address Verification Services \n",
    "Utilize Address Verification Services (E.g. LexisNexis, DOTS, smartystreets) via API calls to validate the Address being entered either via Synchronous Calls. The API could inturn return a much cleaner version of the Address, which could then be consumed and ingested into OpenStreetmaps. \n",
    "\n",
    "#### Pros: \n",
    "1. The Synchronous call to a Address Verification service will serve as an Address Cleansing Step prior to the ingestion into Open Street maps. \n",
    "\n",
    "#### Cons: \n",
    "\n",
    "1. API services might be costly, and OpenStreet might have to pay for those services. This might defeat the purpose of OpenStreet Map being a open source project \n",
    "\n",
    "2. The intermediate third party API call might slow down the Address Intake process from a user perspective as the user will have to wait for the response from the Synchronous Call \n",
    "\n",
    "\n",
    "In addition, it might be a good idea to organize Hackathons/Meetups in different parts of the country to cleanse the dataset in a particular area. E.g A hackathon in the Boston Area could be tasked with the force to cleanse the Boston Area dataset on a periodic basis. Just judging by the sheer volume of datasets for the Boston area alone and extrapolating it to datasets across the world, data wranglers across the world would have a field data cleansing the open street map datasets. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listed Below are some of the references I used \n",
    "* http://wiki.openstreetmap.org/wiki/Browsing\n",
    "* Udacity Lectures \n",
    "* A lot of StackOverflow Threads (everytime I ran into an error in Python and Regular Expressions)\n",
    "* https://docs.python.org/2/library/re.html\n",
    "* https://regexone.com/references/python\n",
    "* https://pymotw.com/2/xml/etree/ElementTree/parse.html#parsing-an-entire-document\n",
    "* http://effbot.org/zone/element-iterparse.htm\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
